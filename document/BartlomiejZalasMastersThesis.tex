\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{listings}
\usepackage{paralist}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{plotmarks}
\usepackage{scrextend}
\usepackage{fourier} 
\usepackage{array}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{pbox}
\usepackage{pgfplots}
\usepackage[tableposition=top]{caption}
\usepackage[rightcaption]{sidecap}
\usepackage{graphicx} 
\usepackage{minted}
\usepackage{adjustbox}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{nameref}
\usepackage{color} 
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage[margin=25mm]{geometry}
\usepackage{sectsty}

\sectionfont{\fontsize{14}{17}\selectfont}
\subsectionfont{\fontsize{13}{16}\selectfont}
\subsubsectionfont{\fontsize{12}{15}\selectfont}


\graphicspath{ {images/} }


\renewcommand\theadalign{cb}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}
\renewcommand{\baselinestretch}{1.1}
 
\let\tempone\itemize
\let\temptwo\enditemize
\renewenvironment{itemize}{\tempone\addtolength{\itemsep}{-0.4\baselineskip}}{\temptwo}

\let\tmpone\enumerate
\let\tmptwo\endenumerate
\renewenvironment{enumerate}{\tmpone\addtolength{\itemsep}{-0.4\baselineskip}}{\tmptwo}

\definecolor{bblue}{HTML}{4F81BD}
\definecolor{rred}{HTML}{C0504D}
\definecolor{ggreen}{HTML}{9BBB59}
\definecolor{ppurple}{HTML}{9F4C7C}
\definecolor{oorange}{HTML}{FFC04D}


\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\author{Bartłomiej Zalas}
\title{System Optimization Based on Performance Evaluation Tests}

\begin{document}


\begin{titlepage}
\begin{center}


\LARGE Wrocław University of Technology\\
\large Faculty of Computer Science and Management\\[7cm]

%\begin{addmargin}[4cm]{0em}
\begin{center}
\textsc{\huge Master's Thesis}\\
\LARGE System Optimization Based on Performance Evaluation Tests \\[1.0cm]
\large Bartłomiej Zalas\\index: 192317\\[3cm]
\end{center}
%\end{addmargin}

\begin{flushright} 
\large Prepared under supervision of:\\Bogumiła Hnatkowska, Ph.D.
\end{flushright}


\vfill

% Bottom of the page
{\large Wrocław, 2017}

\end{center}
\end{titlepage}


\pagebreak
        
\tableofcontents

\pagebreak

{\renewcommand{\abstractname}{Streszczenie}
\begin{abstract}
Z powodu rosnącej popularności sieci Internet zagadnienie wydajności aplikacji internetowych staje się coraz bardziej znaczącym czynnikiem zarówno z perspektywy technicznej jak i biznesowej. Ta praca poświęcona jest zagadnieniu optymalizacji wydajności przy użyciu testów wydajności. Przyjęte podejście polega na optymalizacji konfiguracji pod kątem ruchu sieciowego. Zadanie to realizowane jest za pomocą zaproponowanego systemu -- Automated Performance Evaluation System. System dostosowuje konfigurację trzech elementów - dwóch redundantnych komponentów (dodawanie rekordów do bazy danych partiami oraz pamięć podręczna) oraz konfiguracji serwera (liczba wąt-ków). Zaproponowane rozwiązanie zostało zweryfikowane w przeprowadzonych ekspe-rymentach izolowanych (każdy element weryfikowany osobno), eksperymencie połączo-nym (weryfikacja współpracy między elementami) oraz eksperymentach na losowo wy-generowanym ruchu sieciowym (weryfikacja całego systemu w bardziej realistycznym ruchu sieciowym). Otrzymane wyniki potwierdziły, że zaproponowane rozwiązanie jest w stanie zmniejszyć czas wykonywania żądań. W przypadku izolowanych eksperymentów uzyskana poprawa wynosiła 3\% (pamięć podręczna), 26\% (dodawanie rekordów do bazy danych partiami) oraz 17\% (liczba wątków). W eksperymencie testującym współpra-cę między elementami uzyskana poprawa była równa 38\%. W przypadku eksperymentu z losowo wygenerowanym ruchem sieciowym, system był w stanie zredukować czas wyko-nywania żądania w 9 na 10 przypadków. Poprawa wydajności wyniosła średnio 287.03\%. Wyniki potwierdzają, że w większości przypadków system zadziałał poprawnie. Ukazują one również potencjał zaprezentowanego rozwiązania. 
\end{abstract}}


\begin{abstract}
Due to increasing popularity of the Internet the performance of web applications becomes more and more important factor from the technical and business perspective. This work is designated to the topic of performance optimization with usage of performance evaluation tests. The proposal approach is based on configuration optimization on the basis of given web traffic. This task is realized by the implemented solution - Automated Performance Evaluation System. The system performs configuration adaptation on three elements - two redundant components (batch inserts and cache) and server configuration (threads number). The solution was evaluated by the set of performed experiments: isolated experiments (every adapted element evaluated separately), combined experiment (evaluation of cooperation of adapted elements) and randomly generated traffic experiments (evaluation of the whole system under more realistic scenarios). The obtained results confirmed that proposed solution is able to reduce execution time of requests. In case of isolated evaluation the improvement was equal to 3\% (cache), 26\% (batch) and 17\% (threads number) approximately. In combined evaluation obtained improvement was equal to 38\%. In case of randomly generated traffic the system was able to reduce execution time in 9 from 10 cases. Obtained improvement in the execution time in average was equal to 287.03\%. The results confirms that in most cases the system works correctly. The results also show the potential of the implemented solution.
\end{abstract}
\pagebreak


\section{Introduction} \label{section:introduction}

\subsection{Motivation}

A software optimization is the process of making software more efficient through decreasing of needed resources and decreasing time needed to perform certain operation by software. It can be developed on different levels of software development - source code level, algorithm optimization level, compilation level, runtime level \cite{javaperformance}\cite{optimizationtheory}.

The performance evaluation tests are part of evaluation process, which is developed to ensure that software will operate correctly under desirable number of tasks in production environment \cite{analysisofpet}.   

A software optimization and performance evaluation tests are terms related to each other. Basically, we can say that software optimization is evaluated using performance evaluation tests. 

Motivation for this work was the fact that performance evaluation tests are able only to monitor performance of application. In case when application has a set of parameters influencing performance, people responsible for application performance must adjust these parameters manually or empirically. Solution able to adjust that parameters automatically would be a great facilitation. 

The main goal of this work is to develop a solution, which will be able to optimize web application by optimization of configuration parameters and them automatic adjustment. The first research question that needs to be answered in this work is which and how system parameters influence performance? Next, how to evaluate performance of the system? Another goal is to analyze and provide answers for the question: how performance evaluation tests can help in software optimization - in monitoring and application performance adjustment. Another question which is emerging here is: adjustment of which elements of an web application is the most effective? 

Author of this thesis will try to consider listed problems in domain of web application optimization on runtime level using performance evaluation tests. A software under optimization and performance evaluation will be a web application simulating product management system (developed as a web service) implemented in Java programming language and Spring Framework. The application will be deployed on Tomcat application server. 

\pagebreak
\subsection{Document Structure}

The document is dividend into 7 sections - 
\textit{\nameref{section:introduction}}, 
\textit{\nameref{section:theoreticalbackground}}, 
\textit{\nameref{section:existiontopics}}, 
\textit{\nameref{section:proposedsolution}}, 
\textit{\nameref{section:solutionevaluation}}, 
\textit{\nameref{section:conclusions}}, 
\textit{\nameref{section:references}} and 
\textit{\nameref{section:appendices}}. 

In the first section - \textit{\nameref{section:introduction}} - the motivation, structure of the document and used abbreviations and acronyms are introduced. 

The second chapter \textit{\nameref{section:theoreticalbackground}} introduces theoretical concepts from the domain of this work. The elements considered in this chapter are: performance, key performance indicators, software testing from performance perspective and optimization of the software on different levels. The concepts are considered based on the exiting literature. 
  
The third chapter \textit{\nameref{section:existiontopics}} describes existing problems of the domain of this work. The section is focused on existing problem related to the software performance and existing attempts    dedicated to software optimization, described in the literature of domain.

In the section \textit{\nameref{section:proposedsolution}}, the  analysis of possible solutions related to performance issue is presented. Further part introduces the system which is an attempt of automated performance tuning of the web application.

The section \textit{\nameref{section:solutionevaluation}} presents a set of experiments which where performed in order to evaluate introduced system from the section \textit{\nameref{section:proposedsolution}}. 

The \textit{\nameref{section:conclusions}} section summarizes the work. The research questions introduced in the section \textit{\nameref{section:introduction}} are addressed and concluded. The section also points out possible directions of further work related to the domain of this document. 

In the section \textit{\nameref{section:references}} the literature used to prepare this work is listed.

The last section - \textit{\nameref{section:appendices}} - extends the work by detailed measurements performed during the experiments. 

\pagebreak
\subsection{Abbreviations and Acronyms}
In this work numerous abbreviations and acronyms are used. All are presented and explained below.

\vspace{0.3cm}

\def\arraystretch{1.5}
\begin{tabularx}{\textwidth}{p{1.5cm}X}
API & Application Programming Interface\\ 
CPU & Central Processing Unit\\ 
CDN & Content Delivery Network\\
HTML & HyperText Markup Language\\ 
CSS & Cascading Style Sheets\\ 
SQL & Structured Query Language\\ 
EJB & Enterprise JavaBeans\\ 
ERD & Entity Relationship Diagram\\ 
CRUD & Create, Read, Update and Delete\\ 
GC & Garbage Collector\\ 
JEE & Java Enterprise Edition\\
JDBC & Java DataBase Connectivity\\ 
PET & Performance Evaluation Tests \\ 
APTS & Automated Performance Tuning System \\ 
JVM & Java Virtual Machine\\ 
REST & Representational State Transfer \\ 
KPI & Key Performance Indicator\\
POJO & Plain Old Java Object\\
HTTP & Hypertext Transfer Protocol\\ 
JAR & Java archive\\
\end{tabularx}

\pagebreak

\section{Theoretical Background} \label{section:theoreticalbackground}
Before further considerations fundamental term for this work - performance -  should be defined. 

In H. Sarojadevi's definition, performance relates to time and throughput used by application under certain workload and configuration \cite{petmethodsandtools}. In that definition workload and configuration play a role of input variables to a system/application, when time and throughput are measurable outputs from a system/application. This definition gives information about factors related to performance, but how to recognize good and bad performance?

Ian Molyneaux explains good performance as ability to use application without perceiving any delay or irritation by the end-user \cite{artperformance}. Can delay perceived by an user or irritation be defined? Research \cite{howlong} shows that user expects a response from web application in about 2 seconds. Because of that there exists dependency between performance and popularity (and hence financial result) of website and that is why performance is  considered as critical for web applications from business perspective \cite{architectingperformance}. 

Knowing what performance is  and how to distinguish good and bad performance one must know what indicators are used to measure performance (commonly used name for such indicator is KPI - key performance indicator). Such indicators are part of nonfunctional requirements of application in which, according to \cite{artperformance}, \cite{analysisofpet} and \cite{petmethodsandtools},  most important are:  
\begin{itemize}
\item Response time - The time needed to respond to request.
\item Throughput - The rate of requests to application.
\item Utilization - The percentage of resource usage.
\item Maximum users support - The number of concurrent users supported.
\item Workload profile - The behavior under various different amount of users.
\item Weak points - The places in which application crashes under stress conditions.
\item Scalability - The capability of a system to be enlarged to handle more amount of work.
\end{itemize}

Above indicators are used to evaluate performance of application. The process of software evaluation is called testing. G. Myers defines testing of software as a process of executing a program with the intent of finding errors \cite{arttest}. Andreas Spillner in the book "Software testing foundations" \cite{testfoundations} defines testing as execution and analysis of a application to examine it. Spillner provides also purposes of testing: 
\begin{itemize}
\item Executing a program to find defects
\item Executing a program to measure quality
\item Executing a program to provide confidence 
\item Analyzing a program or its documentation to prevent failures
\end{itemize}

All four purposes are applicable in performance evaluation tests. Defects are found when an application does not behave as it is designed under the given workload. An application quality and performance capabilities are measured. The confidence that an application will serve desired amount of requests is provided. The analysis of a bottlenecks helps to improve a system. 

At the same time we need to have in mind Edsger W. Dijkstra's sentence:
testing shows the presence, not the absence of bugs \cite{set}. That's why  software tests should cover possibly largest set of application use scenarios.  

The test to see whether the application meets documented performance specifications (KPIs) is called performance evaluation test \cite{arttest}. 
In order to process performance evaluation tests a set of activities must be performed. According to Ian Molyneaux \cite{artperformance} following steps should be performed:
\begin{itemize}
\item Nonfunctional Requirements Capture - definition of performance target, performance testing schedule, environment design.
\item Performance Test Environment Build - deployment of the application, configuration and deployment of testing tools and KPI monitoring.
\item Use-Case Scripting - input data requirements, selection of areas to monitor for response time, ensure of correctness or use case replays.
\item Performance Test Scenario Build - creation of test scenarios, and realistic throughput model
\item Performance Test Execution and Analysis - execution of tests and results analysis.
\item Post-Test Analysis and Reporting - collection and analysis of  data from all test runs, reports creation.
\end{itemize}

Accordingly to our objectives different kinds of performance tests may be carried out. In the literature \cite{artperformance},\cite{architectingperformance} and \cite{analysisofpet} three main types of performance testing can be found:
\begin{itemize}
 \item Load testing - To validate application behavior under normal and peak conditions.
 \item Stress testing - To validate application behavior beyond normal load conditions.
 \item Capacity testing - To find a limits of users and transactions which an application is able to handle.
\end{itemize}

Performance evaluation tests can be done manually (by testers playing role of an end user \cite{comparison}) or automatically (using tools which are able to perform workload simulation, monitor indicators and generate reports). Manually testing in most of the cases is time consuming, expensive and hectic. For the better business purpose and to save time and money, automatic testing is required \cite{automaiontools}.

Examples of automated performance testing tools are: Selenium \cite{automaiontools}, Apache JMeter, NeoLoad, LoadRunner and others \cite{analysisofpet}. Common features offered by these tools are \cite{analysisofpet}: 
\begin{itemize}
 \item Heavy load simulation;
 \item Measuring and monitoring performance indicators;
 \item Performance report generation;
 \item Possibility to customize tests by scripts/API;
\end{itemize}

Most important features of automated performance evaluation tests are repeatability (each time application is under the same workload) and autonomy (human role in testing process is limited only to start tests and analyse results). These features give possibility to conduct performance evaluation tests frequently during development phase or can be even included to continuous integration process. Such approach would deliver (practically) instant feedback for developers about impact of new created features on performance \cite{autobugs}. 

Other important features, defined by Molyneaux, which should be considered during choosing testing tool are \cite{artperformance}:
\begin{itemize}
 \item Protocol support - communication between an application and a testing tool
 \item Licensing model - pricing and cost
 \item Scripting effort - manual script changes to reply tests
 \item In-house versus outsourced - way of hosting performance testing tools.
\end{itemize}

Knowing what performance is, how it is measured and how to develop environment for performance evaluation and tests itself, the next important step is optimization of application. 

Singiresu Rao defines optimization as the act of obtaining the best result under given circumstances \cite{optimizationtheory}. In performance oriented development and testing, as in this work, "best result" can be understood as best performance (measured as execution time and resources consumption). "Given circumstances" can be interpreted as workload, current software environment configuration and correctness of chosen algorithms and implementation decisions in an application. 

According to  \cite{javaperformance} software optimization for Java environment can be achieved at many areas, like: compiler optimization, garbage collector tuning, heap memory customization, optimal memory management, threads control, database queries optimization, JVM adjustment, and others.

For the web applications specific types of optimization can be performed.  
The book "Architecting High Performing, Scalable and Available Enterprise Web Applications"  provides information about elements which should be used to optimize a web application. This elements are \cite{architectingperformance}:
\begin{itemize}
 \item HTML optimization - usage of HTML 5 and CSS 3 performance features such as application cache, web SQL, web sockets, CPU optimization. This features should be considered and introduced during application development; 
 \item high performance responsive web design - used to automatically adjust content of a web page to a browser resolution; 
 \item assets optimization - images, videos, JavaScript files, style sheets should be compressed and minimized; 
 \item proper cache configuration - when requested, server should response with a new content of resource only when it was changed from the last request, otherwise "Not Modified" (304) should be returned;
 \item CDN accelerators - geographically distributed servers accelerating  load of web page assets; 
 \item asynchronous and on-demand rendering - web page architecture in which only a page regions with changed content are rerendered after user actions.  
\end{itemize}

Above elements are related to client side and are responsible for data presentation and user experience. They should be taken into consideration during application development and deployment. Because they are not configurable during application runtime and do not influence application core performance, which is responsible for serving data to a client side, they are out of scope of this work. 

\section{Existing Topics Related to Software Performance} \label{section:existiontopics}

\subsection{Performance Related Problems} \label{PerformanceRelatedProblems}
One of the most fundamental problem about applications performance is a nasty habit of turning up late in the application life cycle \cite{artperformance}. In article "Performance Anti-Patterns" \cite{lssrarticle} Bart Smaalders noticed that development teams overloaded by new features development and bug fixing left performance work as an afterthought. The first report from performance is received from developed software versions during beta testing.

To solve and avoid above problem, performance evaluation tests should be repeatable, observable, portable, easily presented, realistic, runnable  \cite{lssrarticle}. Thanks to that, tests can be executed as early as possible during software development cycle, and executed continuously during development providing instant performance feedback to the developers. 

Another problem with web applications served in enterprise environment is multiplicity of manual configuration of parameters needed to create optimal environment for application. Table \ref{glassfishparams} presents common parameters influencing an performance of Java application server \cite{glassfishdoc} \cite{deployerproblem}.  

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{p{3cm}|X}
  \textbf{Parameter} & \textbf{Performance Influence} \\
\hline
Max Thread Pool Size & Increasing this value will reduce HTTP response latency time but may also overload CPU.\\
 
Data Connection Pool Size & Small connection pool has faster access on the connection table, but requests may spend more time in the queue. With large connection pool, requests will spend less (or no) time in the 
queue, but access on the connection table is slower.\\

EJB Pool Size & Large pool wastes memory and  can slow down the system. A very small pool is also inefficient due to contention. \\

JVM Heap Size & Increasing heap size can improve performance, but too high memory usage by JVM can slow down a system.   \\

JVM GC Algorithm & Available algorithms have different influence on pause 
times, throughput and CPU usage.
\end{tabularx}
\end{center}
\caption{\textit{Common Java application server parameters influencing performance}}
\label{glassfishparams}
\end{table}

In article "The deployer's problem: configuring application servers for performance and reliability" \cite{deployerproblem} authors provide, a method for finding good configuration for the application deployment:

\begin{enumerate}
\item For each configuration value of interest, select a reasonable range. 
\item Generate random value from these range.
\item Deploy application with a chosen value.
\item Measure performance.
\item Repeat steps 2-4 until performance of application is satisfying. 
\end{enumerate}

Presented procedure is obviously time consuming - requires manual parameter tuning and application deployment - and deficient because of random parameters value selection. This disadvantages show a need of a tool which will be able to automatically perform above steps and choose the best configuration.

The web applications are characterised by various workload. Such characteristic is dependent from profile and purpose of web application \cite{invariantsworkloads}. This fact should be also taken into consideration during tuning application performance. 

\subsection{Existing Solutions} \label{ExistingSolutions}
In article "Automatic Performance Tuning for J2EE Application Server Systems" \cite{autotuning} authors proposed a solution for automatic application optimization based on \textit{MaxPoolSize} parameter of JBoss application server resulting in 37\% improvement. What is also important, the proposed dynamic configuration  mechanism  was able to improve  performance  and  adapt  to  changes  in  workloads without human intervention. In the article authors also noticed that there is no fully automated framework designed for Java Enterprise application server reconfiguration.

General idea of self-optimization systems was proposed in the article "The Vision of Autonomic Computing" \cite{autoarch}. The main problem introduced by authors is complexity and great number of tunable parameters influencing performance. Also, tuning of one subsystem can have unanticipated effects on the entire system. The proposed solution is an autonomic system. System which will continually seek ways to improve own operation, identifying and seizing opportunities to make themselves more efficient in performance or cost. 

A framework for automatic performance tuning is presented in article "Automatic Performance Management in Component Based Software Systems" \cite{autoframework}. The framework consist of two main modules: monitoring and diagnostic module and application adaptation module. The monitoring and diagnostic part gathers data about performance of application and environment with minimal overhead. Informations about probes (performance data related to method execution and lifecycle
events of EJB) are analyzed and in case of performance problems an alert is raised. When it happens, application module should overcome performance problems by selecting software component which will provide the best possible performance at given run-time context. 

Such concept is called component redundancy and is described deeper in the article "A~framework for using component redundancy for self-adapting and self-optimising component based enterprise systems" \cite{redundancycomponent}. In the article author describes Component Redundancy as presence, at runtime, of multiple  component  variants  providing  identical or equivalent services but with different implementation strategies. Only one of the redundant components providing a service is assigned, at any moment in time, for handling a certain client request for that  service.  Implementation strategies can be added or deleted during runtime. Depending on context (workload, available resources) a suitable component is selected. Selection is based on component description -- a set of attributes which defines in what conditions component usage is the most optimal. These descriptions are then updated at runtime with accurate monitoring information for the actual execution contexts. 

The practical approach of software performance optimization at implementation level is described in the book "Pro Spring 2.5" in chapter "Spring Performance Tuning" \cite{springperformance}. In this chapter authors present most typical performance related problems and explain how to analyze performance and how to find bottlenecks of application written in Spring Framework. The key indicator in presented process is execution time of a method. Major part of text is focused on improving the data access tier by following improvements:  
\begin{itemize}
\item paged data - records from database are queried in chunks, 
\item lazy loading - only records requested by user are returned from query (without related tables), 
\item indexing - database indexes are created in order to improve search, 
\item batched inserts - records are inserted/updated into database in chucks.
\end{itemize}
In the book, authors provide a guide how to increase performance in other areas:
\begin{itemize}
\item view performance - fast rendering of web page,
\item cache - basic usage of method cache is presented.
\end{itemize}
The text provides examples with source code. The authors of the book emphasize that each performance improvement should be based on understanding bottleneck and be adapted to application and users profile.  

\subsection{Summary}

In section \ref{PerformanceRelatedProblems} problems related to software optimization and performance evaluation tests where presented. Conclusion in the form of list presented below shows main raised problems:
\begin{itemize} 
\item Downplaying of performance evaluation tests \cite{artperformance}\cite{lssrarticle}
\item Complexity of performance tuning \cite{glassfishdoc}\cite{deployerproblem}
\item Process of performance tuning is time consuming and manual \cite{deployerproblem}
\item Workload (number of requests to an application) is various \cite{invariantsworkloads}
\end{itemize}

Analysis of existing problems from section \ref{ExistingSolutions} provides also solutions, ideas and hints addressing above problems:
automatic performance tuning \cite{autotuning}\cite{autoarch}\cite{autoframework}, redundant components \cite{redundancycomponent} and suitable implementation decisions \cite{springperformance}. 

Automatic performance tuning addresses the problems with time consuming and complex configuration. Due to fact that application performance is monitored continuously such tuning is performed instantly when needed. Next advantage of such system is that it knows how to achieve the best performance in given circumstances. 

The idea of redundant component gives possibility to develop a solution which will fit well to changeable environment. Managed by automatic performance tuning system may be switched during runtime when system usage characteristic change. 

Suitable performance related decisions impose on developers duty to awareness of performance influence of taken implementation decisions. Developers should be aware of system workload characteristic in production environment and perform performance optimization where possible. 
 
\section{Proposed Solution} \label{section:proposedsolution}

\subsection{Introduction}
On the base of previous chapters author of this work will try to provide solution which will be addressing most of listed problems from chapter \ref{PerformanceRelatedProblems}. Next subsections provide considerations about possible ways to obtain the best possible solutions with justification. Final goal is to implement a system which will demonstrate and evaluate implemented solutions. 

\subsection{Tested Application}   \label{testedApplicaiotn}
\subsubsection{Domain} 
As mentioned in previous chapters performance optimization can be developed in many areas. For this work author will focus on a web application. The application will be a service simulating a product database - basic CRUD operations on products categories will be provided. For simplicity and in order to concentrate on the topic of this work, a graphical user interface will be omitted. Communication with application will be provided in form of REST service.

\subsubsection{Data Model} 
Figure \ref{erd} presents ERD diagram of data model implemented in the tested application. Entity ProductCategory may have zero or many Products. Entity Product may have zero or many ProductOpinions. Each entity has basic a set of attributes (like id, name, date, etc.).  

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{ERD}
\caption{\textit{ERD diagram of the tested application model}}
\label{erd}
\end{figure}

\subsubsection{REST API} 
The tested application REST API provides the following resources:
\vspace{3mm}

\noindent\textbf{Find product category by ID} - returns product category with id defined by parameter \textit{id}.

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & GET\\
  Path: & /find/\{id\}\\
  Parameters: & id (parameter type: path, data type: integer, required: yes)\\
  Response: & Product category in JSON format \\
  \end{tabular} \vspace{5mm}
}

\noindent\textbf{Add new product category} - creates a new category with name defined by parameter \textit{categoryName}. For created category products with opinions are generated. Number of generated products and opinions is chosen randomly from range 1-4. 

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & PUT \\
  Path: & /add/ \\
  Parameters: & categoryName (parameter type: request, data type: string, required: yes)\\
  Response: & Created product category in JSON format  \\
  \end{tabular} \vspace{5mm}
}

\noindent\textbf{Update product category} - updates category defined by parameter \textit{id} by name from parameter \textit{newName}. 

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & POST \\
  Path: & /update/\{id\} \\
  Parameters: & id (parameter type: path, data type: integer, required: yes)\\
              & newName (parameter type: request, data type: string, required: yes)\\
  Response: & Updated product category in JSON format  \\
  \end{tabular} \vspace{5mm}
}

\noindent\textbf{Remove product category} - removes product category with related products and opinions for a given \textit{id}.

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & DELETE \\
  Path: & /remove/\{id\} \\
  Parameters: & id (parameter type: path, data type: integer, required: yes)\\
  Response: & No content  \\
  \end{tabular} \vspace{5mm}
}

\noindent\textbf{Get all product categories paged} - returns product categories with products and opinions for page defined by parameters \textit{page} and \textit{size}.

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & GET \\
  Path: & /findPart/ \\
  Parameters: & page (parameter type: request, data type: integer, required: yes)\\
              & size (parameter type: request, data type: integer, required: yes)\\
  Response: & Product categories in JSON format  \\
  \end{tabular} \vspace{5mm}
  }
  
\noindent\textbf{Get all product categories} - returns all product categories with products and opinions.

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & GET \\
  Path: & /findAll/ \\
  Response: & Product categories in JSON format  \\
  \end{tabular} \vspace{5mm}
}

\noindent\textbf{Remove all product categories} - removes all product categories with related products and opinions.

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & DELETE \\
  Path: & /removeAll/ \\
  Response: & No content  \\
  \end{tabular} \vspace{5mm}
}
  
\noindent\textbf{Create product categories} - generates number of product categories. The number is defined by \textit{size} parameter. For each category also products with opinions are generated. Number of generated products and opinions is chosen randomly from range 1-4. 

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & PUT \\
  Path: & /addCategories/ \\
  Parameters: & size (parameter type: request, data type: integer, required: yes)\\
  Response: & String message  \\
  \end{tabular} \vspace{5mm}
}

\noindent\textbf{Execute CPU consuming task} - starts a task which will simulate high CPU usage. For that purpose generation of 40th number of Fibonacci sequence was chosen. 

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & GET \\
  Path: & /task/ \\
  Response: & No content  \\
  \end{tabular} \vspace{2mm}
}

\noindent\textbf{Execute wait task} - starts a task which will simulate waiting task without CPU consumption. Real life example of such task could be a process, which sends request to an external service (for example by HTTP) and waits for a response. The task "wait" time is defined by the parameter \textit{sleepTime} (in milliseconds). 

{\renewcommand{\arraystretch}{1}
  \begin{tabular}{ll}
  Method: & GET \\
  Path: & /wait/ \\
  Parameters: & sleepTime (parameter type: request, data type: integer, required: yes)\\
  Response: & No content  \\
  \end{tabular} \vspace{5mm}
}
  
Services \textit{Find product category by ID}, \textit{Add new product category}, \textit{Update product category}, \textit{Remove product category} provide basic usage from client perspective. Remaining services were developed for testing purposes (performance tests for different configuration and redundant components from Chapter \ref{realization}).
  
\subsubsection{Implementation Decisions} 
The application will be implemented in Java 1.8. Web based features will be implemented in Spring Framework 4.3.7 and Hibernate 5.0.12. The application server for the application will be Apache Tomcat 8.5.11 served by Spring Bootstrap 1.5.2. Data persistence will be realized with PostgreSQL 9.6 DBMS.  

Above tools are commonly used solutions in a web development. That fact makes implemented solution applicable and easily translated to other web applications based on similar technologies.  

\subsection{System Principles} \label{principles}
The key in every successful project is to define goals and needs at the beginning.
In case of this work it will be realized as system principles definition on the base of  performance problems from chapter \ref{PerformanceRelatedProblems}.

Downplaying of performance evaluation tests -- decision of when, how often or even if perform performance evaluation tests lays in developers way of working. However, performance evaluation tests performed in easy way with minimal effort could encourage developer teams to perform it. 

Complexity of performance tuning - the system should know which set of configuration parameters values will be the best in given situation.  

Process of performance tuning is time consuming and manual - the system should know how to tune configuration to increase performance of application and do it automatically. 

Workload is various - the system should be able to monitor workload of application in the form of incoming requests in order to switch application configuration to such  which will be more suitable in given circumstances. 

On the base of above consideration, the following set of principles of the system can be formulated:

\begin{itemize}
\item The system should perform live monitoring of application
\item The system should be autonomic
\item The system should be able to perform configuration change during application runtime
\item The system should be able to work in production environment
\item The system should have minimal impact on production environment
\end{itemize}

On the base of above principles in the next chapter proposition of system realization will be introduced.  

\subsection{Performance Improvement Realization} \label{realization}

Before system architecture will be introduced a set of elements on which performance tuning will be performed must be introduced.  
Considering existing solutions presented in chapter \ref{ExistingSolutions} author of this work decided to perform performance tuning of the application on the base of two techniques: redundant components and application server configuration parameters. 

\subsubsection{Research Environment}

In order to make the right choices about performance improvement realization, a set of experiments must be performed. 
For that purpose 10000 product categories with related entities were generated. Each product category may have from 1 to 4 products. Each product may have from 1 to 4 opinions. Exact number of related entities was chosen randomly during generation. In summary number of created entities for experiments is as follows:
\begin{itemize}
\item Product category: 10000 entities
\item Product: 22269 entities
\item Product opinion: 49406 entities
\end{itemize}  

Execution time of methods was measured by aspect oriented class. The class wraps all methods provided by REST API by execution time measurement calculated as difference between two dates: start date (measured before method execution) and end date (measured after method execution). The execution time was logged and persisted in a time series database (InfluxDB) in order to enable further analysis. In order to make execution time more accurate each experiment was run 10 times and final execution time was calculated as arithmetic mean of 10 values obtained in individual experiments. 

Required workload was simulated by tests scenarios generated in Gatling Load and Performance testing engine \cite{gatling}. In order to evaluated tunable elements, the following test scenarios were developed:
\begin{itemize}
\item \textbf{Squeeze Application} - generates high CPU usage of the tested application by firing 100 parallel requests to CPU consuming resource - "Execute CPU consuming task". 
\item \textbf{Frequently Changing Model} - generates traffic with alternate firing  "Update product category" and "Find product category by ID" resources in 10 parallel requests.
\item \textbf{Immutable Model} - generates traffic based on "Find product category by ID" resource (without changes on data model) in 10 parallel requests.
\item \textbf{Insert} - generates 20 sequenced requests to "Add new product category" resource. 
\end{itemize}

Author of this work decided to took into consideration following candidates for tunable elements: paging, cache, batched inputs. The decision was based on facts that all of mentioned candidates are able to increase performance of the  tested application domain and it is possible to develop a switchable equivalent for the given candidate. 

In the first experiment different paging mechanism were analyzed. The execution time for each mechanism was measured by summing up execution time needed to retrieve all 10000 product categories from the tested application.   

In the second experiment, cache influence on performance was analyzed. Methods "Update product category" and "Find product category by ID" where tested with and without cache enabled against scenarios: Frequently Changing Model and    Immutable Model. 

In the next experiment, a solution with batched inserts was tested. Batch with size equal to 10 entities was used. Total time of execution during scenario Insert was compared with solution without batch. 

The last experiment tested influence of number of threads used by application server. In order to simulate complex tasks execution by each thread Squeeze Application scenario was used against 1, 10 and 100 threads. 

\subsubsection{Redundant Components}
Redundant components, by definition, provide the same services but with different a implementation. From the performance perspective that idea can be used to implement component which will provide the same services with different performance characteristic. What is also important, usage of redundant components must be justified - there must exist circumstance where given redundant component will offer better performance than another redundant component. 

Paging is a database query realization in which on a single select query only part (page) of records is returned instead of querying all records from database. Table \ref{pagingcomponents} shows comparison of two proposed redundant components: Paging and No Paging.  
\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{p{3.5cm}|X|X}
  \textbf{Category} &\textbf{Paging} & \textbf{No Paging} \\
\hline
Query all records & Multiple queries (slower) & One query (faster) \\
User satisfaction & High & Low\\
Top performance circumstances & When impossible to query all records by No Paging component & When possible to query all records at once\\
\end{tabularx}
\end{center}
\caption{\textit{Paging redundant components comparison}}\label{pagingcomponents}
\end{table}

\begin{figure}[!htb]
\centering
\begin{tikzpicture}
        \begin{axis}[
            symbolic x coords={{10000}, {5000}, {2500}, {1250}, {625}},
            xtick=data,
            bar width=1cm,
            x=2cm,
            ymin=0,
            enlarge x limits={abs=1cm},
            xlabel={Page Size [records]},
            ylabel={Execution time [ms]},
            ymajorgrids = true,
          ]
            \addplot[ybar,style={bblue,fill=bblue,mark=none}] coordinates {
                ({10000}, 46)
                ({5000},  60)
                ({2500},  94)
                ({1250},  114)
                ({625},   127)
            };
        \end{axis}
\end{tikzpicture}

\caption{\textit{Execution time comparison for different page sizes for 10000 product categories}} \label{fig:pagesizetime}

\end{figure}

As shown in Figure \ref{fig:pagesizetime}, the bigger page size is the smaller execution time is. It is related to the fact that every query to a database generates additional overhead. 
In general the No Paging component is faster when it is possible to query all records at once. Possible to query means, that query will not need more resources (memory, CPU) than is available. 
The Paging component must perform multiple queries to a database in order to query all existing records. From the practical point of view in presentation layer only first page of records is needed when user entering a web page. Otherwise, long time of No Paging component execution will decrease user satisfaction level.  

The paging idea is more related to presentation layer and user satisfaction. Moreover, when number of records in a database is possible to query at once, No Paging component will be always faster to query all records. Because of that author of this work decided to reject paging as redundant component. 

Next redundant component candidate is cache. There exist many levels of abstraction on which cache can be applied to a web application -- request level cache, method level cache, global data cache. For goals of this work the method-level cache of service responsible for managing database operations was chosen - in general such operations are time consuming. The cache saves returned value from a method and uses input parameters as the cache key. Table \ref{cachecomponents} shows comparison of two proposed redundant components: Cache (methods are cached) and No Cache (cache is disabled).
\begin{table}[!htb]
\begin{center}
\begin{tabular}{l|c|c}
  \textbf{Category} &\textbf{Cache} & \textbf{No Cache} \\
\hline
Preferable parameters characteristic & Repeatable & Varied \\
Preferable resources & Immutable & Mutable\\
Overhead & Yes & No\\
\end{tabular}
\end{center}
\caption{\textit{Cache and No Cache redundant components comparison}}\label{cachecomponents}
\end{table}
 
Table \ref{cachecomponents} shows that Cache and No Cache redundant components have different characteristics. In workload in which requests are repeatable (given set of parameters is used more than once) and there are no frequent changes in state of requested resources (no updates and inserts into a database) preferred redundant component is Cache. When there is no repeatable requests and requested resources are frequently changed, usage of cache generates only additional overhead. In such case No Cache component should be used. On the base of above considerations Cache/No Cache components could be considered as a proper redundant component from performance perspective and will be implemented in the system. 

Last redundant component taken into consideration are batched inserts. The idea is to gather objects which are to be inserted into a database and insert them in one query instead of few separate ones. This minimize query overhead. In case of this work inserts batch is realized in form of collection of the objects in which insert query is fired when collection is full.   
Table \ref{batchedcomponents} shows comparison of two proposed redundant components: Batched (records to insert are batched) and Direct (direct inserts into a database as separate queries).

\begin{table}[!htb]
\begin{center}
\begin{tabular}{l|c|c}
  \textbf{Category} &\textbf{Batched} & \textbf{No Batched} \\
\hline
Query execution time & Faster & Slower \\
Query effects & Delayed & Instant\\
Efficiency & High & Low\\
\end{tabular}
\end{center}
\caption{\textit{Batched and Direct redundant components comparison.}}\label{batchedcomponents}
\end{table}

\begin{figure}[!htb]
\centering
\begin{tikzpicture}
        \begin{axis}[
            symbolic x coords={Batched, Direct},
            xtick=data,
            bar width=1cm,
            x=2cm,
            ymin=0,
            enlarge x limits={abs=1cm},
            xlabel={Redundant component},
            ymajorgrids = true,
            ylabel={Execution time [ms]}
          ]
            \addplot[ybar,style={bblue,fill=bblue,mark=none}] coordinates {
                (Batched,  386)
                (Direct,   563)
            };
        \end{axis}
\end{tikzpicture}
\caption{\textit{Execution time comparison for Batched and Direct redundant components for batch with size = 10 and number of records = 20}} \label{fig:batchedtime}
\end{figure}

The components comparison in the Table \ref{batchedcomponents} shows different characteristics of both redundant components. Batched one is in general more efficient and faster than No Batched. However, the process of batching records introduces delays in real effect of batched statement (equal to time in which given records sf "frozen" in the batch). In proposed implementation batched statement will be fired when batch is full. Both facts lead to conclusion that Batched component should be used only in circumstances when such delay is acceptable by users or when number of inserts into a database is so high that efficiency improvement is needed. Another thing is the more frequent inserts into a database (faster filling up of batch) the smaller delay. Therefore Batched component should be used only in case when number of inserts into a database is very high. Otherwise Direct component should be used. Above considerations justify choice of Batched/Direct redundant components to be used in the system.   

\subsubsection{Application Server Configuration Parameters}

Configuration of application server is crucial for performance of application. Application servers provide wast number of parameters which can be configured. Such parameters may be slightly different depending on vendor but in most cases configuration provides possibility to adjust: thread number, JDBC connection pools, EJB pools, JVM configuration \cite{glassfishdoc}. 

Author of this work decided to take into consideration 2 possible configurations: EJB pool size and thread number of application server. Both configurations have direct influence on the number of requests which are handled by application and should provide high performance improvements in the application domain. 

EJB pooling mechanism reduces the time needed for creating new bean instances (beans are created on start up and kept in a pool) and support parallel request handling. In case of this work, the tested application is developed in Spring framework with singleton scoped beans (there is one instance of bean for entire application) - therefore EJB pool configuration has no effect on application performance (singleton beans are not pooled). Influence of EJB pool size on performance of enterprise application was checked in other work \cite{autotuning} where authors already proved that tuning EJB pool size has positive influence on a JEE application performance. Taking into account this arguments EJB pool will be not implemented in the system. 

Number of threads has the key meaning in handling incoming requests into an application server. When number of threads is less than a number of incoming requests in parallel, not handled part will be waiting in the queue until threads will be free again. Too large number of threads results in increased CPU usage and slower performance. This fact is documented in the Figure \ref{fig:threads}. For 100 of incoming parallel requests three different configuration of thread number were tested: 1 thread, 10 threads, 100 threads. As one can see, mean response time was high when too many parallel threads were executed or when the number of threads was not sufficiently large to handle incoming requests.    

\pgfplotstableread[row sep=\\,col sep=&]{
    category           & t1    & t10    & t100  \\
    Min response time  & 1161.3  & 3549.8   & 33610.6 \\
    Max response time  & 85256 & 33587.1  & 37619.3 \\
    Mean response time & 43260.1 & 18705.5  & 36441.2 \\
    }\mydata
\begin{figure}[!htb]
\centering
\begin{tikzpicture}
    \begin{axis}[
            ybar,
            bar width=.6cm,
            width=0.8\textwidth,
            height=.6\textwidth,
            legend style={at={(0.5,-0.15)},
                anchor=north,legend columns=-1},
            symbolic x coords={Min response time,Max response time,Mean response time},
            xtick=data,
            ymin=0,ymax=90000,
            ylabel={Execution time [ms]},
            enlarge x limits={abs=2cm},
            ymajorgrids = true,
            legend image code/.code={\draw[#1, draw=none] (0cm,-0.15cm) rectangle (0.5cm,0.2cm);},
        ]
        \addplot[style={bblue,fill=bblue,mark=none}] table[x=category,y=t1]{\mydata};
        \addplot[style={rred,fill=rred,mark=none}] table[x=category,y=t10]{\mydata};
        \addplot[style={ggreen,fill=ggreen,mark=none}] table[x=category,y=t100]{\mydata};
        \legend{1 thread, 10 threads, 100 threads}
    \end{axis}
\end{tikzpicture}
\caption{\textit{Mean response time from 100 incoming paraller requests}} \label{fig:threads}
\end{figure}


Number of threads has large impact on application performance and it configuration will be implemented in the system. 

\subsubsection{Performance Improvement Realization Summary}

Above considerations contain justification about accepted or declined candidates for performance tuning in the system. Finally, two redundant components (batched imports, cached methods) and one application server configuration parameter (threads number) was chosen.


\subsection{System Architecture}

\subsubsection{Automated Performance Tuning System}

In order to enable performance tuning in the tested application on the base of solutions described in Chapter \ref{realization} the Automated Performance Tuning System (APTS) is introduced. The APTS is based on principles formulated in Chapter \ref{principles}. Main functionality of the system is to enable automated  performance tuning of the tested application in real time. 

\subsubsection{APTS Components}

As presented on Figure \ref{componentapts}, the system is composed of the following components: \textit{Workload Generator}, \textit{APTS Manager} which includes \textit{PET Cases} and \textit{Decision Module}, \textit{InfluxDB Service}, \textit{Configuration Service}, \textit{Resource Monitoring Service}, \textit{Aspect Oriented Application Monitoring} and \textit{Tested Application}.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{APTSComponentDiagram}
\caption{\textit{Component diagram of the APTS}}
\label{componentapts}
\end{figure}

The goal of \textit{Workload Generator} is to provide simulation of network traffic to the tested application. Traffic simulation is realized as HTTP requests generation based on Gatling library \cite{gatling}. The \textit{Workload Generator} is independent from the APTS and it communicates directly to the tested application. Thanks to that features, the module can be easily disabled in production environment where real traffic is analyzed.  

The \textit{APTS Manager} is the main part of the system. It is responsible for enabling performance tuning by performing performance evaluation tests and reporting performance issues raised by tests to \textit{Decision Module}. 

The \textit{PET Cases} component contains performance evaluation scenarios written with help of PET Framework (see Chapter \ref{framework}). The tests monitor the tested application in specified schedule, gather monitored KPIs from \textit{InfluxDB Service} and asserts KPIs values. In case of failed assertion performance issue is raised.

The \textit{Decision Module} is responsible for performance issue analysis and performance tuning. This feature is realized in the form of rule based system in which performance issues are conditions and performance tuning decisions are actions.   

The \textit{InfluxDB Service} hosts InfluxDB \cite{influxdb} time series database which provides HTTP API for querying and writing data. The database stores performance measurements which are generated by the tested applications and required by the performance evaluation tests. In order to visualize data stored in the database in the form of readable graphs the Grafana \cite{grafana} platform was integrated with the InfluxDB. 

The \textit{Configuration Service} is an external service where performance tuning configuration is stored (active redundant component and current application server thread number). The \textit{Configuration Service} provides REST API which enables different parts of the system to check and change current configuration of the tested application. The possible configuration values are presented in the Table \ref{table:configurationvalues}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{c|c|X}
\textbf{Parameter} &\textbf{Available values} & \textbf{Description} \\\hline
\multirow{2}*{CACHE} & NO\_CACHE & Cache is handled by the No Cache component\\\cline{2-3}
      & CACHED    & Cache is handled by the Cached component   \\ \hline
\multirow{2}*{BATCH} & DIRECT    & Batch is handled by the No Batched component   \\\cline{2-3}
	  & BATCHED   & Batch is handled by the Batched component   \\ \hline
\multirow{3}*{THREADS} & T10     & Application server will use 10 threads\\ \cline{2-3}
        & T20     & Application server will use 20 threads\\ \cline{2-3}
        & T30     & Application server will use 30 threads\\ \hline
\end{tabularx}
\end{center}
\caption{\textit{The possible configuration values of the \textit{Configuration Service}}}\label{table:configurationvalues}
\end{table}	


The \textit{Resource Monitoring Service} is responsible for providing information about current resource consumption of the machine on which the \textit{Tested Application} is stored. The component is managed by the \textit{APTS Manager} which starts monitoring by sending another HTTP request when performance evaluation tests are started. When the tests are finished monitoring is stopped by the manager by sending HTTP request. Data about resources consumption are written into the \textit{InfluxDB Service} in real time. 

The \textit{Aspect Oriented Application Monitoring} provides information about execution time of methods provided by the \textit{Tested Application} REST API. Since the monitoring is implemented in Spring Aspect Oriented Programming \cite{springaop} it is transparent for the \textit{Tested Application} - in other words - the \textit{Tested Application} does not know about the monitoring component. Informations about execution time are stored in the \textit{InfluxDB Service}.

The \textit{Tested Application} component represents the tested application described      wider in Chapter \ref{testedApplicaiotn}.  The application uses the \textit{Configuration Service} in order to store and enable tuning of configuration.


\subsubsection{APTS Deployment} \label{section:aptsdeploy}

The APTS, as presented on Figure \ref{deploymentapts}, is deployed on \textit{Client Workstation}, \textit{Application Server} and \textit{Service Server}.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{APTSDeploymentDiagram}
\caption{\textit{Deployment diagram of the APTS}}
\label{deploymentapts}
\end{figure}



The \textit{Application Server} node is designated to host tested application and is independent from the rest of the system - in case of \textit{Configuration Service} unavailability predefined default configuration is used. When \textit{InfluxDB} is offline, no performance measurements are stored. In order to provide information about current resource consumption on the node the \textit{Resource Monitoring} service is also deployed on the node \textit{Application Server}. 

The \textit{Service Server} node holds additional services used by the APTS. This node was separated from the \textit{Application Server} node in order to minimize overhead of the APTS on the tested application.

The node called \textit{Client Workstation} is any node (with JRE installed) which holds the \textit{APTS Manager}. 


\subsubsection{PET Framework} \label{framework}
 
In order to implement performance evaluation tests cases in readable, consistent and easy way, the PET Framework was implemented. The framework is independent from the APTS what means that JAR with the framework can be used in any other project. 

The framework main features are:
\begin{itemize}
\item POJO based
\item PET cases configured by annotation
\item PET cases schedule (duration and frequency of a test case execution)
\item Performance issues reporting at runtime 
\item Test cases written in JUnit \cite{junit} style
\end{itemize}

Work with the framework is based on POJOs which provides lightness (no additional third party software or middleware is required) and flexibility (any Java code can be used during work with the framework) of the solution. PET cases are  configured using Java annotations what makes configuration readable and convenient. Each PET case is run in configured schedule what enables continuous monitoring of the tested software. Thanks to asynchronous implementation any issues which are reported during tests can be obtained immediately when appear. PET cases convention was inspired on popular and successful JUnit Framework - each person familiar with JUnit should easily understand conventions of PET Framework.       

Main elements on which PET Framework is based are annotations, assertions and performance issues. 

Annotations are used to mark POJOs to be interpreted as PET cases and to configure test case. In the PET Framework two annotations are available:

\begin{table}[!htb]
\def\arraystretch{1.5}
\begin{tabularx}{\textwidth}{p{1.4cm}X}
\textbf{@Pet} 	  & Annotation used to mark a class to be interpreted as PET case container. \\ 
\textbf{@PetCase} & Annotation used to mark a method to be interpreted as PET case. Available parameters are presented in Table \ref{petcase}. To explain parameters more deeply let consider an example in which parameters are configured as follows: \textit{monitorIntervalInSec} = 30, \textit{durationInSec} = 120. These values mean that the test case will be executed in every 30 seconds in period of time equal to 120 seconds (2 minutes) i.e., a test case will be executed 4 times and after 2 minutes execution will terminate. Assuming that a test case will be started at 12:00:00 hour, executions will be started at 12:00:00, 12:00:30, 12:01:00 and 12:01:30. 
\end{tabularx}
\end{table}

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|c|c|X}
  \textbf{Parameter name} &\textbf{Type} & \textbf{Default value} & \textbf{Description} \\
\hline
			\textit{enabled} & boolean & true & Indicates if PET case should be run.\\
			\textit{monitorIntervalInSec} & integer & 30 & Interval between test case executions in seconds.\\
			\textit{durationInSec} & integer & 3600 & Maximum duration of the test case in seconds.\\
			\textit{delayInSec} & integer & 0 & Delay after which the tests case will start in seconds.\\
\end{tabularx}
\end{center}
\caption{\textit{Available parameters of @PetCase annotation}}\label{petcase}
\end{table}			

In order to evaluate the current KPIs against specified values a set of assertions was introduced. In case of assertion failure, the PET Framework converts exception to the performance issue. Signatures of assertions are presented on Listing  \ref{assertions}. As presented, for the performance evaluation tests, three assertions where implemented (to ensure that KPI is less, greater of equal to a specified value).  Parameters used in assertions are: 
\begin{itemize} 
\item \textit{metric} - identifier of KPI used in performance issue.
\item \textit{currentValue} - current value of KPI measured on an tested application
\item \textit{limit}/\textit{expectedValue} - value to which currentValue is compared to. In case when limit is exceeded or value is different than expected assertion error is thrown.
\end{itemize}

\begin{listing}[ht]\begin{minted}[fontsize=\footnotesize]{java}

void assertKpiLessThan(String metric, Double currentValue, Double limit);

void assertKpiGreaterThan(String metric, Double currentValue, Double limit);

void assertKpiEqual(String metric, Double currentValue, Double expectedValue);
\end{minted}
\caption{PET assertions signatures} \label{assertions}
\end{listing}

An important part of the PET Framework is the \textit{PerformanceIssue} entity. When PET assertion fails or monitoring state needs to be announced, the entity \textit{PerformanceIssue} containing all necessary information about failure is used. Informations contained in \textit{PerformanceIssue} entity are later analyzed by the Decision Module. Class diagram of the \textit{PerformanceIssue} is presented on the Figure \ref{issue}.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.45\textwidth]{PerformanceIssueClassDiagram}
\caption{\textit{PerformanceIssue class diagram}} \label{issue}
\end{figure}
 
Attributes of the class are: 
\begin{itemize}
\item \textit{metric} - identifier of monitored KPI.
\item \textit{reason} - information about reason why issue is reported or current state of monitored KPI.
\item \textit{message} - human readable message with explanation about issue.
\end{itemize}

Sample PET case written with usage of the PET Framework is presented on the Listing \ref{case}. Annotations \textit{@Pet} and \textit{@PetCase} are used to inform framework that this class is implementation of PET case. Method  \textit{meanExecutionTime\_shouldBeOnAcceptableLevel} is interpreted as PET case implementation and is configured by annotation \textit{@PetCase} with definied two attributes - \textit{monitorIntervalInSec} with assigned value 60 and \textit{durationInSec} with assigned value 3600. This configuration means that PET case will be executed in every minute by the period of 1 hour. In the body of the method assertion \textit{assertKpiIsLessThan} is defined which simply checks if mean execution time was on acceptable level - below 1.2 second. For simplicity process of retrieving execution time measurement was represented as method  \textit{getExecutionTimeFromLastMinute()}.  


\begin{listing}[ht]\begin{minted}[fontsize=\footnotesize]{java}
/*
 * imports
 */
@Pet
public class SamplePETCase {

    @PetCase(monitorIntervalInSec = 60, durationInSec = 3600)
    public void meanExecutionTime_shouldBeOnAcceptableLevel() throws Exception {
        
        double acceptableExecutionTime = 1200.;
        
        double meanExecutionTime = getExecutionTimeFromLastMinute();
        
        assertKpiLessThan("executionTime", meanExecutionTime, acceptableExecutionTime);
    }
}
\end{minted}
\caption{Sample PET case written in the PET Framework} \label{case}
\end{listing}

In order to run tests cases the PET Framework provides runner (PetCaseRunner) which will run all tests cases marked with proper annotations from the given package. Because of the need of continuous monitoring of the reported performance issues, the runner is implemented as a thread and provides asynchronous API to retrieve new reported issues. The Listing \ref{petrunner} presents a sample usage of the PetCaseRunner. 

\begin{listing}[ht]\begin{minted}[fontsize=\footnotesize]{java}
PetCaseRunner runner = new PetCaseRunner("com.zalas.masterthesis.apts.pet.cases");
runner.start();

while(runner.isAlive()) {
    Thread.sleep(1000);
    System.out.println(runner.getNewIssues());
}
\end{minted}
\caption{Sample usage of the PetCaseRunner} \label{petrunner}
\end{listing}

As presented on the Listing \ref{petrunner}, a new instance of PetCaseRunner class is created. The package name where tests cases are located is given as a parameter to the constructor. Next, the instance of the runner is started - new thread is started. All PET cases are run on the defined schedule within this thread. In order to retrieve information about performance issues reported by test cases which are run by runner thread method, \textit{getNewIssues()} is called within "while" loop in 1 second interval. In this sample,  retrieved issues are printed out to standard output. In case of APTS, performance issues are delegated to the Decision Module, where they are analyzed. When all test cases are finished the runner thread is terminated.    

\subsubsection{Implemented PET Cases} \label{dm}

 The performance evaluation tests cases in the APTS are responsible for monitoring the current state (execution time, resource consumption) of the tested application and reporting performance issues which are analyzed by the \textit{Decision Module}. The monitoring is realized on the base of data gathered in the \textit{InfluxDB Service}.  Tuning of the tested application is realized using elements chosen in the Chapter \ref{realization}. In order to enable tuning of the considered elements from the Chapter \ref{realization} following PET cases was implemented: \textit{MeanExecutionTimePET}, \textit{MonitorInsertsLevelPET},  \textit{MonitorTrafficProfilePET}.
 
 The \textit{MeanExecutionTimePET} test case monitors mean execution time from a given interval of the methods provided by the tested application REST API. If mean execution time is greater than specified value, the performance issue is reported. 

The \textit{MonitorInsertsLevelPET} test case monitors how many database inserts are performed on the tested application. If the number of inserts per seconds is greater the a given threshold, the reported performance issue contains information about high level of inserts. Otherwise, the performance issue contains information about low level of inserts. The performance issue with above information is reported after every execution of test case. 

The \textit{MonitorTrafficProfilePET} test case monitors traffic profile which comes to the tested application. There are two possible values of traffic profile - mutable and immutable. A mutable traffic profile means that the state of the tested application is changed (by inserts or updates operations). An immutable traffic profile means that requests to the application do not change the state of the tested application (read requests). The traffic profile is considered as immutable when the number of mutable requests is equal to zero in measured period of time. The performance issue with information about traffic profile is reported after every execution of test case.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|p{4cm}|X}
\textbf{Test Case} & \textbf{Metric Name of Performance Issue} & \textbf{Possible Reasons of Performance Issue} \\ \hline
MeanExecutionTimePET & executionTime & EXCEEDED \\\hline
MonitorInsertsLevelPET & insertsLevel &LOW, HIGH \\\hline
MonitorTrafficProfilePET & trafficProfile & MUTABLE, IMMUTABLE \\
\end{tabularx}
\end{center}
\caption{\textit{Possible values of the Performance Issue reported by the specific test case.}}\label{testcasescomp}
\end{table}

The below listing presents definitions of the reasons of performance issues implemented in the performance evaluation test cases. The chosen values are adapted to the testing environment. 
\begin{itemize}
\item[EXCEEDED (executionTime)] - mean execution time of requests from the last 10 seconds was greater than 1.2 second. 
\item[LOW (insertsLevel)] - the number of insert requests per second in the last 10 seconds was  less than 10. 
\item[HIGH (insertsLevel)] - the number of insert requests per second in the last 10 seconds was not less than 10.
\item[MUTABLE (trafficProfile)] - in the last 10 seconds at least one mutable request was executed.
\item[IMMUTABLE (trafficProfile)] - in the last 10 seconds none mutable request was executed.
\end{itemize}

\subsubsection{Decision Module} \label{dm}

The role of the \textit{Decision Module} is to perform performance tuning actions on the base of reported performance issues, current configuration of the tested application and current resource consumption on the node where the tested application is deployed (see the Figure \ref{decisionmodule}). 

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{APTSFlow}
\caption{\textit{Performing Tuning Action by Decision Module}} \label{decisionmodule}
\end{figure}

Decision module is triggered by reported performance issues in order to analyze above factors and perform an action. Then, the \textit{Decision Module} retrieves information about current configuration of the tested application and resource consumption on the tested application node from \textit{Configuration Service} and \textit{Resource Monitoring Service}. Next, a decision about the best rule is preformed and action related to the best rule is executed (see the Figure \ref{dmsequence}). 

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{DecisionModuleSequenceDiagram}
\caption{\textit{Sequence diagram presenting the Decision Module workflow}} \label{dmsequence}
\end{figure}

Decision about which action will be performed is made using a rule based system. Implemented rule based system  consists of two parts: set of the rules and inference engine. The set of rules is created with the aim of performance tuning of the tested application. Each rule fulfills contract defined in the form of the \textit{Rule} interface presented on the Listing \ref{irule}.

\begin{listing}[ht]\begin{minted}[fontsize=\footnotesize]{java}
public interface Rule {
    boolean isRuleApplicable(
    	IssueToHandle issueToHandle, 
    	ApplicationConfiguration currentConfiguration
    );
    void executeAction();
    int getPriority();
}
\end{minted}
\caption{Rule interface} \label{irule}
\end{listing}

The interface enforce on rules to define the following methods: 
\begin{itemize}
\item isApplicable - method evaluates boolean condition in order to decide if the current rule is applicable for the reported performance issue.   
\item executeAction - action to perform when a given rule is typed by the system as the best rule.   
\item getPriority - method returns priority of a given rule - the higher number - the higher priority. The priority allows to prioritize rules in case when more than one rule was qualified by the system as applicable. In such case a rule with the highest priority is chosen as the best one. 
\end{itemize}

In order to perform automated tuning of the tested application based on elements defined in the Chapter \ref{realization} the following rules are implemented: \textit{Enable Batch Rule}, \textit{Disable Batch Rule}, \textit{Enable Cache Rule}, \textit{Disable Cache Rule}, \textit{Switch To 10 Threads Rule}, \textit{Switch To 20 Threads Rule}, \textit{Switch To 30 Threads Rule}. Below, all implemented rules are presented in details.  

\vspace{5mm}

%-------------ENABLE BATCH RULE START-----------------
\noindent\textbf{Enable Batch Rule} \\
The rule enables batching mechanism provided by the Batched redundant component in the tested application when the number of  inserts performed by the tested application is classified by the MonitorIsertsLevelPET test case as "high". Enabling batch mechanism should decrease total time of insert requests. Details of the rule are presented in the Table \ref{ruleenablebatch}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|X}
\textbf{Applicability Condition:} & \textbf{isIssueRelatedInsertsLevel} \textit{AND} \newline
\textbf{isInsertsLevelHigh} \textit{AND} \newline
\textbf{isBatchDisabled} \\ \hline

\textbf{Subconditions Definition:} & \textbf{isIssueRelatedInsertsLevel} -  Reported performance issue metric is equal to "InsertsLevel"  \\
& \textbf{isInsertsLevelHigh} - Reported performance issue status is equal to "High" \\
& \textbf{isBatchDisabled} - Current application configuration parameter "BATCH" is equal to "DIRECT" \\ \hline

\textbf{Action:} & Change the configuration of parameter "BATCH" to value "BATCHED" \\ \hline
\textbf{Priority:} & 1\\
\end{tabularx}
\end{center}
\caption{\textit{Details of the Enable Batch Rule}} \label{ruleenablebatch}
\end{table}

%-------------ENABLE BATCH RULE END-----------------
\vspace{5mm}
%-------------DISABLE BATCH RULE START-----------------
\noindent\textbf{Disable Batch Rule} \\
The rule disables batching mechanism provided by Batched redundant component. Instead, the Direct redundant component is used.  The rule is applicable when the number of inserts performed by the tested application is classified by the MonitorIsertsLevelPET test case as "low". Disabling batch mechanism should reduce delay introduced by the batch mechanism and provide instant insert into the database. Details of the rule are presented in the Table \ref{ruledisablebatch}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|X}
\textbf{Applicability Condition:} & \textbf{issueRelatedToInsertsLevel} \textit{AND} \newline
\textbf{isInsertsLevelLow} \textit{AND} \newline
\textbf{isBatchEnabled} \\ \hline

\textbf{Subconditions Definition:} & \textbf{issueRelatedToInsertsLevel} -  Reported performance issue metric equals to "InsertsLevel"  \\
& \textbf{trafficProfileIsLow} - Reported performance issue status is equal to "Low" \\
& \textbf{batchAlreadyEnabled} - Current application configuration parameter "BATCH" is equal to "BATCHED" \\ \hline

\textbf{Action:} & Change the configuration of parameter "BATCH" to value "DIRECT"\\ \hline
\textbf{Priority:} & 1\\
\end{tabularx}
\end{center}
\caption{\textit{Details of the Disable Batch Rule}} \label{ruledisablebatch}
\end{table}
%-------------DISABLE BATCH RULE END-----------------
\vspace{5mm}
%-------------ENABLE CACHE RULE START-----------------
\noindent\textbf{Enable Cache Rule} \\
The rule enables cache mechanism provided by the Cache redundant component in the tested application when traffic profile incoming to the tested application is classified by the MonitorTrafficProfilePET test case as "immutable". Enabling cache mechanism should decrease time of requests execution. Details of the rule are presented in the Table \ref{ruleenablecache}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|X}
\textbf{Applicability Condition:} & \textbf{isIssueRelatedTrafficProfile} \textit{AND} \newline
\textbf{isTrafficImmutable} \textit{AND} \newline
\textbf{isCacheDisabled} \\ \hline

\textbf{Subconditions Definition:} & \textbf{isIssueRelatedTrafficProfile} -  Reported performance issue metric is equal to "TrafficProfile"  \\
& \textbf{isTrafficImmutable} - Reported performance issue status is equal to "Immutable" \\
& \textbf{isCacheDisabled} - Current application configuration parameter "CACHE" is equal to "NO\_CACHE" \\ \hline

\textbf{Action:} & Change the configuration of parameter "CACHE" to value "CACHED" \\ \hline
\textbf{Priority:} & 1\\
\end{tabularx}
\end{center}
\caption{\textit{Details of the Enable Chache Rule}} \label{ruleenablecache}
\end{table}
%-------------ENABLE CACHE RULE END-----------------
\vspace{5mm}
%-------------DISABLE CACHE RULE START-----------------
\noindent\textbf{Disable Cache Rule} \\
The rule disables cache mechanism provided by the Cache redundant component. Instead the  No Cache redundant component is used. The rule is applicable when traffic profile incoming to the tested application is classified by the MonitorTrafficProfilePET test case as "mutable". Disabling cache mechanism should eliminate overhead related to the cache mechanism which in mutable environment is needless. Details of the rule are presented in the Table \ref{ruledisablecache}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|X}

\textbf{Applicability Condition:} & \textbf{isIssueRelatedTrafficProfile} \textit{AND} \newline
\textbf{isTrafficMutable} \textit{AND} \newline
\textbf{isCacheEnabled} \\ \hline
\textbf{Subconditions Definition:} & \textbf{isIssueRelatedTrafficProfile} -  Reported performance issue metric is equal to "TrafficProfile"  \\
& \textbf{isTrafficMutable} - Reported performance issue status is equal to "Mutable" \\
& \textbf{isCacheEnabled} - Current application configuration parameter "CACHE" is equal to CACHED" \\ \hline

\textbf{Action:} & Change the configuration of parameter "CACHE" to value "NO\_CACHE" \\ \hline
\textbf{Priority:} & 1\\
\end{tabularx}
\end{center}
\caption{\textit{Details of the Disable Cache Rule}} \label{ruledisablecache}
\end{table}
%-------------DISABLE CACHE RULE END-----------------
\vspace{5mm}
%-------------SWITCH TO 10 THREADS RULE START-----------------
\noindent\textbf{Switch to 10 Threads Rule} \\
The rule represents the lowest possible threads number available in the APTS implementation. The rule switches configuration responsible for the number of threads used by application server on which the tested application is deployed to 10 threads. The rule is applicable when mean execution time defined in MeanExecutionTimePET test case is exceeded and CPU usage on the application node is high.  Switching to low threads number should help reduce the CPU usage and hence reduce total execution time of the requests. Details of the rule are presented in the Table \ref{rules10}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|X}

\textbf{Applicability Condition:} & \textbf{isExecutionTimeExceeded} \textit{AND} \newline
\textbf{isThreadsNumberDifferent} \textit{AND} \newline
\textbf{cpuUsageIsHigh} \\ \hline

\textbf{Subconditions Definition:} & \textbf{isExecutionTimeExceeded} -  Reported performance issue metric is equal to "ExecutionTime"  \\
& \textbf{isThreadsNumberDifferent} -  Current application configuration parameter "THREADS" is not equal to "T10" \\
& \textbf{cpuUsageIsHigh} - Current CPU usage on the application node is greater than 80\% \\ \hline

\textbf{Action:} & Change the configuration of parameter "THREADS" to value "T10" \\ \hline
\textbf{Priority:} & 1\\
\end{tabularx}
\end{center}
\caption{\textit{Details of the Switch to 10 Threads Rule}} \label{rules10}
\end{table}
%-------------SWITCH TO 10 THREADS END-----------------
\vspace{5mm}
%-------------SWITCH TO 20 THREADS RULE START-----------------
\noindent\textbf{Switch to 20 Threads Rule} \\
The rule represents the middle possible value of threads number available in the APTS implementation. The rule switches configuration responsible for the number of threads used by application server on which the tested application is deployed to 20 threads. Conditions of applicability of rule are as following: mean execution time defined in MeanExecutionTimePET test case is exceeded and CPU usage is high or low for respectively low or high threads number. Switching to the higher threads number should help reduce the execution time of requests which are waiting in the queue to be handled by the application. Switching to the lower threads number should help reduce the CPU usage and hence reduce total execution time of the requests. In order to perform threads number switching by one level the rule has set higher priority in comparison to other threads related rules (in example: when current thread number is 10, and rules "Switch to 20 Threads Rule" and "Switch to 30 Threads Rule" are evaluated as applicable - rule "Switch to 20 Threads Rule" will be chosen because of the higher priority). Details of the rule are presented in the Table \ref{rules20}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|X}
\textbf{Applicability Condition:} & \textbf{isExecutionTimeExceeded} \textit{AND} \newline
(\textbf{isThreadNumber10} \textit{AND} \textbf{cpuUsageIsLow}) \textit{OR} \newline
(\textbf{isThreadNumber30} \textit{AND} \textbf{cpuUsageIsHigh}) \\ \hline

\textbf{Subconditions Definition:} & \textbf{isExecutionTimeExceeded} -  Reported performance issue metric is equal to "ExecutionTime"  \\
& \textbf{isThreadNumber10} -  Current application configuration parameter "THREADS" is equal to "T10" \\
& \textbf{isThreadNumber30} -  Current application configuration parameter "THREADS" is equal to "T30" \\
& \textbf{cpuUsageIsLow} - Current CPU usage on the application node is not greater than 80\% \\ \hline
& \textbf{cpuUsageIsHigh} - Current CPU usage on the application node is greater than 80\% \\ \hline

\textbf{Action:} & Change the configuration of parameter "THREADS" to value "T20" \\ \hline
\textbf{Priority:} & 2\\
\end{tabularx}
\end{center}
\caption{\textit{Details of the Switch to 20 Threads Rule}} \label{rules20}
\end{table}
%-------------SWITCH TO 20 THREADS END-----------------

\vspace{5mm}
%-------------SWITCH TO 30 THREADS RULE START-----------------
\noindent\textbf{Switch to 30 Threads Rule} \\
The rule represents the highest possible threads number available in the APTS implementation. The rule switches configuration responsible for the number of threads used by application server on which the tested application is deployed to 30 threads. The rule is applicable when mean execution time defined in MeanExecutionTimePET test case is exceeded and CPU usage on the application node is low. Switching to high threads number should help reduce the execution time of requests which are waiting in the queue to be handled by the application. Details of the rule are presented in the Table \ref{rules30}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|X}
\textbf{Applicability Condition:} & \textbf{isExecutionTimeExceeded} \textit{AND} \newline
\textbf{isThreadsNumberDifferent} \textit{AND} \newline
\textbf{cpuUsageIsLow} \\ \hline

\textbf{Subconditions Definition:} & \textbf{isExecutionTimeExceeded} -  Reported performance issue metric is equal to "ExecutionTime"  \\
& \textbf{isThreadsNumberDifferent} -  Current application configuration parameter "THREADS" is not equal to "T30" \\
& \textbf{cpuUsageIsLow} - Current CPU usage on the application node is not greater than 80\% \\ \hline

\textbf{Action:} & Change the configuration of parameter "THREADS" to value "T30" \\ \hline
\textbf{Priority:} & 1\\
\end{tabularx}
\end{center}
\caption{\textit{Details of the Switch to 30 Threads Rule}} \label{rules30}
\end{table}
%-------------SWITCH TO 30 THREADS END-----------------

\vspace{10mm}
%-------------DO NOTHING RULE START-----------------
\noindent\textbf{Do Nothing Rule} \\
The rule is returned by the implemented rule based system when there is no applicable rules for the current system state.  The rule is created as consequence of the Null Object Pattern \cite{nop} used in the implementation of the system and is not stored in the set of available rules of the rule based system. Execution of the rule does not change anything - implementation of the method \textit{executeAction()} is empty. Details of the rule are presented in the Table \ref{ruledonothing}.

\begin{table}[!htb]
\begin{center}
\begin{tabularx}{\textwidth}{l|X}
\textbf{Applicability Condition:} & N/A \\ \hline
\textbf{Action:} & N/A \\ \hline
\textbf{Priority:} & 0\\
\end{tabularx}
\end{center}
\caption{\textit{Details of the Do Nothing Rule}} \label{ruledonothing}
\end{table}
%-------------DO NOTHING THREADS END-----------------

The second part of the implemented rule based system is the inference engine. The role of the engine is to choose the best rule from the set of provided rules and perform action related to chosen rule. In order to achieve this the algorithm presented in the Figure \ref{dmalgorithm} is used. 

\begin{figure}[!htb]
\centering
\includegraphics[width=0.4\textwidth]{DecisionModuleActivityDiagram}
\caption{\textit{The algorithm of selecting best rule by implemented rule based system}} \label{dmalgorithm}
\end{figure}

First step of the algorithm is to choose only applicable rules from the set of rules. In case when there are applicable rules - rule with the highest priority is chosen as best rule. If there is no applicable rules "Do Nothing Rule" is chosen.

\subsubsection{APTS Manager} \label{manager}

The APTS Manager is a component which is responsible for initiating, run and coordinate PET Cases and Decision Module components. The sequence of actions performed by the manger during performance tuning of the application is presented in the Figure \ref{manager}.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{ManagerSequenceDiagram}
\caption{\textit{Sequence Diagram presenting the APTS Manager workflow}} \label{manager}
\end{figure}


When performance tuning is started, the manager starts the \textit{Resource Monitoring Service}. Next, performance evaluation test cases are executed by the \textit{PetCaseRunner}. The manager monitors any reported performance issues in real time and delegates every new reported issue to the \textit{Decision Module} where decision about performance tuning is made. When all PET cases performed by the \textit{PetCaseRunner} are finished the manager stops the \textit{Resource Monitoring Service} and terminates. 

\section{Solution Evaluation}  \label{section:solutionevaluation}

In this section the Automated Performance Tuning System will be evaluated. The evaluation of the system should confirm practical advantages of the proposed solution and reveal effectiveness under given circumstances. 

\subsection{Experiments Environment}

All experiments conducted in the section will be performed on environment with the specification described in the following subsections.

\subsubsection{System Deployment} \label{section:systemdeployment}

In order to evaluate the APTS, the environment introduced in the Chapter \ref{section:aptsdeploy} was created.
The created system is deployed on a machine with the following specification:
\begin{itemize}
\item Processor: Intel Core i3-3120M CPU @ 2.50GHz, 2500 Mhz, 2 Cores, 4 Logical Processors
\item Physical Memory (RAM): 12,0 GB
\item Operating System: Microsoft Windows 10 Education
\item Java version: 1.8.0\_121
\end{itemize}

Deployment of the system components with nodes properties is presented in the Figure \ref{figure:deployment:evaluation}. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{ExperimentsEnvironemtDiagram}
\caption{\textit{Deploymet diagram presenting the evaluation environemnt}} \label{figure:deployment:evaluation}
\end{figure}

The \textit{APTS Manager} and the \textit{Workload Generator} are run directly on the \textit{Client Workstation}. 

In order to simulate \textit{Application Server} node and \textit{Service Server} node introduced in the Chapter \ref{section:aptsdeploy} two virtual machines where introduced - the \textit{Service Server Virtual Machine} and \textit{Application Server Virtual Machine}. This solution minimizes overhead introduced by the APTS on the tested application. A network interfaces on the virtual machines where configured to enable HTTP communication between the system nodes. 

Because change of threads number in Tomcat Server requires restart of the server the Dispatcher component was introduced. The component is composed of the Dispatcher deployed on a Jetty Server and three copies of the tested application deployed on different instances of Tomcat Server configured with different number of threads. The servers with the application instances are listening on different ports. The role of the Dispatcher is to dispatch all incoming requests into a right port according to current configuration on the provided by the Configuration Server.


\subsubsection{PET Cases Configuration}

During the experiments dedicated to evaluate given mechanisms (cache, batch, threads) only PET case related to a given mechanism is enabled (i.e. MonitorTrafficProfilePET, MonitorInsertsLevelPET, MeanExecutionTimePET respectively) in order to prevent distortions. 

The configuration of the all PET cases is presented in the Table \ref{evaluationtestconf}. The duration is adapted to 2 minutes test scenario. The additional time and delay is introduced due to practical reasons - traffic simulation engine needs few seconds to be initiated before it starts to generate requests. The monitoring interval provides adequate time between individual test run.

\begin{table}[!htb]
\begin{center}
\begin{tabular}{l|l}
\textbf{durationInSec} & 180 \\ \hline
\textbf{monitorIntervalInSec} & 10 \\ \hline
\textbf{delayInSec} & 10\\
\end{tabular}
\end{center}
\caption{\textit{The PET cases configuration during the experiments}} \label{evaluationtestconf}
\end{table}


\subsubsection{Initial Application Configuration}

The initial configuration provided by the \textit{Configuration Service} during the experiments evaluating the APTS is presented in the Table \ref{table:initconfiguration}. As presented, by default cache and batch are disabled, the application server threads number is set to middle value, i.e. 20 threads. 

\begin{table}[!htb]
\begin{center}
\begin{tabular}{l|l}
\textbf{Configuration Parameter} & \textbf{Value} \\ \hline
CACHE & NO\_CACHE \\ \hline
BATCH & DIRECT\\ \hline
THREADS & T20\\
\end{tabular}
\end{center}
\caption{\textit{The initial configuration of the tested application during the experiments}} \label{table:initconfiguration}
\end{table}

\subsubsection{Database}

The functionality provided by the tested application is based on database related operations. In order to verify how the number of records stored in the database influences execution time of requests, an experiment which result is presented in the Appendix \ref{appendix:influencedb} was performed. 

For the experiment three types of requests were chosen - \textit{Find product category by ID} (find), \textit{Update product category} (update) and \textit{Add new product category} (add). Each of this three requests is used during the system evaluation and uses database during execution. The requests were executed on the database with different number of product categories - 1 and 10000. The obtained results show that the difference between these two cases was less than 1\%. Moreover, in case of update requests the execution time was faster on the database with 10000 records. That result is explained by the fact that the Postgres uses cache to avoid disk I/O operation which are time consuming. The Postgres cache causes that the database content has minimal influence on the performed experiments until it is stored in the Postgres cache. 

In order to simulate production environment for the need of the experiments the database was filled with the dummy entities in the following amount: 

\begin{itemize}
\item Product category: about 10000 entities
\item Product: about 20000 entities
\item Product opinion: about 50000 entities
\end{itemize}

\subsubsection{Grafana}

The Grafana is an open source software for time series analysis. During the experiments it is used to enable live monitoring of the performance metrics stored in the InfluxDB database. 

For the need of experiments 2 types of graphs were configured in the Grafana environment - Execution Time and CPU usage. 

The Execution Time graph presents the duration of each request in time. To obtain that information from InfluxDB, the metrics presented in the Figure \ref{figure:grafanametricsexecution} were configured. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{grafanametricsexecution}
\caption{\textit{Execution Time graph - configured metrics}} \label{figure:grafanametricsexecution}
\end{figure}

In the queries the following names were used: 
\begin{itemize}
\item[duration] - a field responsible for storing execution time of request
\item[execution\_time] - is name of measurement
\item[\$timeFilter] - Grafana parameter allowing to narrow statistics to visible area
\item[methodName] - a field responsible for storing information about request type
\end{itemize} 

The query marked as "F" is used to retrieve information about all requests. 
In order to provide more information about particular type of executed request during the experiments based on the randomly generated scenarios, the matrices  "A", "B", "C", "D" and "E" were added to the graph. 

The CPU Usage graph presents percentage CPU usage in time. To obtain that information from InfluxDB the metrics presented in the Figure \ref{figure:grafanametricscpu}. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{grafanametricscpu}
\caption{\textit{CPU Usage graph - configured metrics}} \label{figure:grafanametricscpu}
\end{figure}

The metric is configured by a simple query which retrieves CPU usage from the field "cpu" from the "resources" measurement.  

\subsection{Cache Management Evaluation} \label{section:cachemenagementevaluation}

The experiments presented below are focused on Cache and No Cache redundant components. The APTS should manage mentioned components in a way which provides the optimal performance of the tested application.

\subsubsection{Traffic Simulation Scenario} \label{trafficcachesim}

The most important concern during evaluation of the solution is traffic simulation. All decisions made by the Decision Module are based on analysis and monitoring of the incoming traffic to the application. For the case of redundant components related to cache management the traffic simulation presented in the Figure \ref{trafficcache} was implemented.

\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}
\draw[->] (0,0) -- (10,0);
\foreach \x in {1,5,9}
\draw(\x cm,3pt) -- (\x cm, -3pt);

\draw (1,0) node[below=3pt] {$0$};
\draw (5,0) node[below=3pt] {$60$};
\draw (9,0) node[below=3pt] {$120$};

\draw[fill=bblue] (5,0) rectangle (9,1);
\draw (7,0) node[above=6pt, align=center, white] {IMMUTABLE requests};

\draw[fill=ggreen] (1,0) rectangle (5,1);
\draw (3,0) node[above=6pt, align=center] {MUTABLE requests};

\draw (10.8,0) node {$time [s]$};
\end{tikzpicture}
\end{center}
\caption{\textit{Traffic simulation scenario for the cache management evaluation}} \label{trafficcache}
\end{figure}

The simulation takes 120 seconds and is divided into the two phases. In the first 60 seconds alternating GET ("Find product category by ID") and UPDATE ("Update product category") requests are generated in order to simulate mutable state of the application. In the second phase only GET ("Find product category by ID") requests are generated what simulates immutable state of the application. In the first phase 60 GET and 60 UPDATE requests are generated with constant injection step 2 requests (1 GET and 1 UPDATE) per second. In the second phase 60 requests are generated with constant injection step 1 request per second. All requests are designated to the same product category (the same ID parameter). 

From the high level perspective, in the first phase enabled cache introduces only unnecessary overhead (all cached GET requests are immediately "cleared" from cache by UPDATE requests). In the second phase enabled cache provides benefit - only the first request is executed and response for the rest is taken from cache.  

\subsubsection{Evaluation Samples} \label{evaluationsamplescache}

In order to evaluate the optimization of the performance offered by the APTS three  samples must be introduced: 
\begin{itemize}
\item Cache - control sample in which cache is enabled in the tested application
\item No Cache - control sample in which cache is disabled in the tested application
\item Adapted - system evaluation sample in which cache is manged by the APTS
\end{itemize}

The goal of the control samples (\textit{Cache} and \textit{No Cache}) for this experiment is to provide reference to which results generated during \textit{Adapted} sample evaluation can be compared. On the all three samples the same test scenario is performed. 



\subsubsection{Experiment} 

The experiment was repeated 10 times for all three samples introduced in the Chapter \ref{evaluationsamplescache} what in total gives 30 repetitions of the experiment.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{cacheEval2}
\caption{\textit{Adapted sample experiment run - Grafana screenshot}} \label{figure:cachegraphanascreen}
\end{figure}

The run of the experiment on the \textit{Adapted} sample is presented on the screenshot from Grafana platform in the Figure \ref{figure:cachegraphanascreen}. In the graph intensified mutable traffic can be recognized during the first half of the experiment. After that, less intense immutable traffic is generated. After the about 15 seconds of immutable traffic execution time of the particular requests is reduced what can be proven by the log messages generated by the system:

The APTS Manager:

\noindent\fbox{\parbox{\textwidth}{\setstretch{1.0} \texttt{\footnotesize 
23:56:28.775 (...) Chosen rule: EnableCacheRule \\
Action: enable cache
}}}\vspace{1mm}

The Configuration Service:

\vspace{1mm}\noindent\fbox{\parbox{\textwidth}{\texttt{\footnotesize
2017-05-16 23:56:28.790  (...) Configuration of CACHE changed to value: CACHED
}}}\vspace{1mm}

The tested application:

\vspace{1mm}\noindent\fbox{\parbox{\textwidth}{\texttt{\footnotesize
2017-05-1.23:56:29.848 (...) Used redundant component: CACHED
}}}\vspace{1mm}
 

\subsubsection{Results} 

The summary of the execution time during experiments are presented in the Table \ref{cacheCompResutls}. The table presents average execution time of the requests for each sample during the experiments. The row \textit{Improvement} shows the percentage improvement between control samples and \textit{Adapted} sample. In other words the row presents how the APTS decreased the execution time of requests. The value of the \textit{Improvement} column was computed using the formula \eqref{eq:cache}. 

\begin{equation} \label{eq:cache}
Improvement = \frac{ControlSample - AdaptedSample}{ControlSample} \cdot 100\%
\end{equation}

All results from performed experiments are available in the Appendix \ref{appendixcacheeval}. 

\begin{table}[!htb]
\begin{center}
\begin{tabular}{l|c|c|c}
 & \textbf{Cache} & \textbf{No Cache} & \textbf{Adapted} \\ \hline
Average Execution Time &  19.75ms & 19.84ms & 19.08ms\\ \hline
Improvement &  3.38\% & 3.84\% & N/A\\ 
\end{tabular}
\end{center}
\caption{\textit{Comparison between control and adapted samples}} \label{cacheCompResutls}
\end{table}

\subsubsection{Interpretation} 

The results show that usage of the APTS gives slight improvement in execution time in comparison to samples where Cache or No Cache samples where used. The obtained improvement in both cases is around 3.5\%. The value is not impressive because of two facts: small cache overhead and relatively small cache improvement. 

Small cache overhead in the tested application is caused by relatively small time needed to manage cache comparing it to the time of the whole request time execution. As is shown in the Appendix \ref{appendixspringconcurentmap}, time needed to execute Put and Clear methods by the \textit{ConcurrentMapCache} class (which is used by the Spring framework cache mechanism) is equal to 0.61 ms for Put and 0.56 ms for Clear. For simplicity let us assume that operation on \textit{ConcurrentMapCache} takes 0.6 ms. Taking into account that the average execution time of the whole request during the experiment takes about 19 ms and during the experiment (for the Cache sample) there was one operation on \textit{ConcurrentMapCache} per request, one can see that cache overhead is a 3\% of the execution time of whole request.  

Relatively small cache improvement in the implemented scenario is caused by small difference between cached and not cached method execution during the experiments. As is shown in the Appendix \ref{appendixcachenotcached} - cached requests are about 20\% faster than not cached requests. 

\subsubsection{Experiment Conclusions} 


Performed experiments prove that performance optimization provided by the APTS has positive effect on the execution time of the requests. The improvement obtained during the experiments is equal to 3.38\% (comparing to enabled cache) and 3.84\% (comparing to disabled cache). The value of the improvement is relatively small but taking into account that a web applications must handle thousands of the requests - even such small improvement matters in longer perspective.  

What is also important, the experiment legitimate implementations of the redundant components with different performance characteristic in a software. In the experiment, cache mechanism was used, but there exist many other possible implementations - for example: stateless and statefull EJB beans, different data storage mechanism (database, in-memory, file system) or different sorting algorithms. Common thing for all mentioned examples is fact, that for a given component there exist alternative which in some circumstances provides better performance.     

 

\subsection{Batch Management Evaluation}

The experiments presented below are focused on Batched and Direct redundant components. The APTS should ensure proper management for both components in order to provide optimal performance of the tested application with the highest possible level of user satisfaction.  

\subsubsection{Traffic Simulation Scenario} \label{batchsimulationscenario}

For the experiment evaluating the batch management provided by the APTS, the traffic simulation presented in the Figure \ref{trafficbatch} was implemented.

\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}
\draw[->] (0,0) -- (10,0);
\foreach \x in {1,5,9}
\draw(\x cm,3pt) -- (\x cm, -3pt);

\draw (1,0) node[below=3pt] {$0$};
\draw (5,0) node[below=3pt] {$60$};
\draw (9,0) node[below=3pt] {$120$};

\draw[fill=rred] (5,0) rectangle (9,1);
\draw (7,0) node[above=6pt, align=center, white] {10 requests / s};

\draw[fill=oorange] (1,0) rectangle (5,1);
\draw (3,0) node[above=6pt, align=center] {1 requests / s};

\draw (10.8,0) node {$time [s]$};
\end{tikzpicture}
\end{center}
\caption{\textit{Traffic simulation scenario for the batch management evaluation}} \label{trafficbatch}
\end{figure}

The simulation takes 120 seconds and is divided into the two phases. In the first 60 seconds INSERT ("Add new product category") request are generated with the constant injection step 1 request per second. In the 60th second injection step is increased to 10 requests per second. 

In other words the simulation in the first phase makes usage of batch undesirable (records are inserted when batch is full, so with batch with size = 10 creates 10 seconds of delay what is not acceptable from user perspective). In the second phase because of high inserts rate usage of batch is desirable (with batch with size = 10 introduced delay is only 1 second what is acceptable solution).  
 
\subsubsection{Batch Size} 
During the experiments batch with size equal to 10 was used. 


\subsubsection{Evaluation Samples} 

In order to measure how the APTS influences performance of the tested application, the control sample must be introduced. The control sample checks performance of the application with disabled APTS performance tuning. 

For the need of batch measurement experiments, only control sample with disabled batch is introduced. As mentioned in the Chapter \ref{batchsimulationscenario}, implemented traffic scenario with enabled batch introduces not acceptable delay between request execution and actual data persistence in the first phase of simulation. 

During the experiment the same simulation scenario is performed on both samples - control and evaluation (sample with the APTS performance tuning enabled) sample.

\subsubsection{Experiment} 

The experiment was repeated 10 times for both control and evaluation samples what in total gives 20 repetition of the experiment. 

In the Figure \ref{batchevaluationscreencontrol} and \ref{batchevaluationscreenevaluation} screenshots from Grafana platform with experiment run for both samples is presented. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{batchCtrl}
\caption{\textit{Control sample experiment run - Grafana screenshot}} \label{batchevaluationscreencontrol}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{batchEval}
\caption{\textit{Evaluation sample experiment run - Grafana screenshot}} \label{batchevaluationscreenevaluation}
\end{figure}

In the graphs, measured execution time during the experiment is presented. In both graphs the first phase of the experiment looks nearly the same - requests are generated with injection step 1 request per second. In the second phase (after 60 seconds) of the experiment increased number of request (measure points) can be observed. In that phase traffic generator switched injection step to 10 requests per second. In the Figure \ref{batchevaluationscreenevaluation}, after 15 seconds from the start of the second phase, regular picks can be observed. This pick marks the moment of the batch flushing - the batch was full and entities were inserted into the database in one query. Thereby, the Figure \ref{batchevaluationscreenevaluation} proves that batch management developed by the APTS was performed correctly. 

\subsubsection{Results} \label{section:batchresults}

The summary of the execution time during the experiments are presented in the Table \ref{batchCompResutls}. The table presents average execution time in particular phases and total time of the experiment. The last row presents difference between the experiments made on different samples. The value of difference is calculated using the formula \eqref{eq:batch}. 

\begin{equation} \label{eq:batch}
Difference = \frac{|ControlSample - AptsAdaptation|}{ControlSample}\cdot 100\%
\end{equation}


\begin{table}[!htb]
\begin{center}
\begin{tabular}{l|ccc}
                 &  \multicolumn{3}{c}{\textbf{Execution time [ms]}} \\
                 &  \textbf{Phase I } & \textbf{Phase II} & \textbf{Total} \\ \hline
Control sample   &  30.0 & 31.5 & 31.4 \\ 
APTS adaptation  &  30.9 & 22.5 & 23.2 \\ \hline
Difference       &  3\% & 28.6\% & 26.1\% \\ 
\end{tabular}
\end{center}
\caption{\textit{Comparison between control and adapted by APTS sample}} \label{batchCompResutls}
\end{table}

The data presented in the Table \ref{batchCompResutls} are visualized in the Figure \ref{batchEvalResutlsGraph}.  All results from performed experiments are available in the Appendix \ref{appendixbatchevaluation}. 


\pgfplotstableread[row sep=\\,col sep=&]{
    category  & control& apts  \\
    Phase I   & 30.0   & 30.9 \\
    Phase II  & 31.5   & 22.5  \\
    Total     & 31.4   & 23.2  \\
    }\mydata
\begin{figure}[!htb]
\centering
\begin{tikzpicture}
    \begin{axis}[
            ybar,
            bar width=.6cm,
            width=0.8\textwidth,
            height=.5\textwidth,
            legend style={at={(0.5,-0.15)},
                anchor=north,legend columns=-1},
            symbolic x coords={Phase I,Phase II,Total},
            xtick=data,
            ymin=0,ymax=35,
            ylabel={Average execution time [ms]},
            enlarge x limits={abs=2cm},
            ymajorgrids = true,
            legend image code/.code={\draw[#1, draw=none] (0cm,-0.15cm) rectangle (0.5cm,0.2cm);},
        ]
        \addplot[style={bblue,fill=bblue,mark=none}] table[x=category,y=control]{\mydata};
        \addplot[style={ggreen,fill=ggreen,mark=none}] table[x=category,y=apts]{\mydata};
        \legend{Control sample, APTS adaptation}
    \end{axis}
\end{tikzpicture}
\caption{\textit{Average execution time in particular phases during the experiment for control and APTS adapted sample}} \label{batchEvalResutlsGraph}
\end{figure}

\subsubsection{Interpretation} 

The results presented in the Chapter \ref{section:batchresults} demonstrate performance improvement delivered by the APTS solution. During the first phase execution time for both samples are nearly the same because in both cases batch was disabled. The crucial part of the experiment was the second phase where number of inserts was increased and batch was enabled by the APTS. The obtained improvement in this phase was equal to 28.6\%. The improvement in the second phase influences reduction of the total time of the experiment by 26.1\%. 

Such good improvement obtained during the second phase of the experiment was possible because of the implemented batch mechanism provided by Batch redundant component. In this mechanism the number of time consuming inserts into the database is reduced. Assuming that batch size is equal to S - only S-th request generates query into the database. For the rest of the requests entity is stored into the batch and immediate response is returned by the tested application. 

\subsubsection{Experiment Conclusions} 

Performed experiments prove ability of the APTS to optimize performance of the tested application by the batch management. The average improvement obtained during the experiments, in total time perspective, was equal to 26.1\%. 

The experiments performed on the example of the batch mechanism demonstrates implementation and usage of the redundant components with special characteristic. The characteristic in which one of the redundant component can be used only in specified circumstances and when used brings significant improvement on the performance. More precisely: in the cache mechanism evaluated in the Chapter \ref{section:cachemenagementevaluation}, both redundant components can be used in every circumstances (in not optimal way), in the batch mechanism batch can only be enabled when inserts level is high enough to reduce delay introduced by the batching.  


\subsection{Threads Number Evaluation}

The experiments presented below are focused on threads management provided by the APTS. The system should manage threads number of the application server where the tested application is deployed in a way which provides the optimal performance of the tested application. 

\subsubsection{Traffic Simulation Scenario}

For the experiments related to the threads management provided by the APTS the traffic simulation presented in the Figure \ref{figure:trafficthreads} was implemented. 

\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}
\draw[->] (0,0) -- (10,0);
\foreach \x in {1,5,9}
\draw(\x cm,3pt) -- (\x cm, -3pt);

\draw (1,0) node[below=3pt] {$0$};
\draw (3,0) node[below=3pt] {$30$};
\draw (5,0) node[below=3pt] {$60$};
\draw (9,0) node[below=3pt] {$120$};

\draw[fill=bblue] (5,0) rectangle (9,1);
\draw (7,0) node[above=-3pt, align=center, white] {CPU consuming \\ requests};

\draw[fill=ggreen] (1,0) rectangle (3,1);
\draw (2,0) node[above=-3pt, align=center] {Wait \\ requests};

\draw (10.8,0) node {$time [s]$};
\end{tikzpicture}
\caption{\textit{Traffic simulation scenario for the threads management evaluation}} \label{figure:trafficthreads}
\end{center}
\end{figure}

The simulation takes 120 seconds and is divided into the two phases. In the first 30 seconds 180 requests to resource "Execute wait task" are generated. The parameter \textit{sleepTime} for generated requests in this phase is set to 5000 (5 seconds). Injection step of requests is constant (6 requests per seconds were used in order to generate more requests than the application is able to handle using the default configuration). The gap between the first and the second phase is introduced due to fact that the first phase may vary in execution time between both samples. The gap gives necessary time to finish the first phase before the second phase will be started. The second phase starts in 60th second of the simulation and takes 60 seconds. During this phase 60 requests to resource "Execute CPU consuming task" are generated with constant injection step (1 request per second). 

From the high level perspective, the particular request from the first phase of the simulation blocks one thread of the application server to 5 seconds without CPU consumption. Threads which are blocked during simulation wait in the queue until threads will be available again. That is why during this phase, the number of threads should be configured to maximal value - low CPU usage allows to increase the number of threads, high number of threads reduces the number of requests waiting in the queue. During the second phase generated requests cause high CPU usage. In order to optimize execution time of requests during this phase, the number of threads should be reduced.    

 
\subsubsection{Evaluation Samples}

The experiments related to number of threads management were developed on two samples:
\begin{itemize}
\item Control sample - in which performance tuning is disabled and the tested application has default  configuration (see Table \ref{table:initconfiguration}).  
\item Adapted sample - in which performance tuning by the APTS is enabled and the initial configuration is default (see Table \ref{table:initconfiguration}).
\end{itemize}

The goal of the control sample is to provide a reference to which sample adapted by the APTS will be compared. For both samples the same traffic simulation will be performed.
 
\subsubsection{Experiment} 

The experiment was repeated 10 times for both control and adapted samples what in total gives 20 repetition of the experiment. In the Figure \ref{figure:threads:screen:control} and \ref{figure:threads:screen:adapted} screenshots from Grafana platform with experiment run for both samples is presented.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{threadsCtrl}
\caption{\textit{Control sample experiment run - Grafana screenshot}} \label{figure:threads:screen:control}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{threadsEval}
\caption{\textit{APTS adapted sample experiment run - Grafana screenshot}} \label{figure:threads:screen:adapted}
\end{figure}

In the graphs, CPU consumption and requests execution time are presented. On the first sight both phases are easy to recognize.

The first phase is different on both graphs. During experiment on the control sample, the first phase was longer (time between first and last request execution). Moreover, gaps indicating lack of free threads on the server are visible. Such gaps are also visible on the graph presenting adapted sample, but here there are visible only at the beginning of the experiment (where the same initial threads number configuration was used). In the rest of the first phase gaps disappear what means that the number of threads was increased by the APTS. 

The second phase on both graphs starts with rapid increase of the CPU usage. Due to that, execution time of the following requests increases. In the middle of the second phase, adapted sample was able to reduce execution time of the requests by decreasing the number of threads for the application server.  

The threads management provided by the APTS during the experiment is also visible in the components logs presented below.

The APTS Manager:

\noindent\fbox{\parbox{\textwidth}{\setstretch{1.0} \texttt{\footnotesize 
17:37:12.272 (...) Chosen rule: SwitchTo30ThreadsRule \\
action: threads switched to 30 \\
(...)\\
17:38:12.184 (...) Chosen rule: SwitchTo20ThreadsRule \\
action: threads switched to 20\\
(...)\\
17:38:22.379 (...) Chosen rule: SwitchTo10ThreadsRule\\
action: threads switched to 10
}}}\vspace{1mm}

The Configuration Service:

\vspace{1mm}\noindent\fbox{\parbox{\textwidth}{\texttt{\footnotesize
2017-05-20 17:37:12.466  (...) Configuration of THREADS changed to value: T30\\
2017-05-20 17:38:12.299  (...) Configuration of THREADS changed to value: T20\\
2017-05-20 17:38:22.494  (...) Configuration of THREADS changed to value: T10
}}}\vspace{1mm}

The Dispatcher:

\vspace{1mm}\noindent\fbox{\parbox{\textwidth}{\texttt{\footnotesize
17:37:29.053 (...) Current threads configuration : T30\\
17:37:29.055 (...) Used port: 8030\\
(...)\\
17:38:15.380 (...) Current threads configuration : T20\\
17:38:15.382 (...) Used port: 8020\\
(...)\\
17:38:57.301 (...) Current threads configuration : T10\\
17:38:57.314 (...) Used port: 8010
}}}\vspace{1mm}

\subsubsection{Results} 

The summary of performed experiment is presented in the Table \ref{table:threads:summary}. The summary presents average values computed from 10 repetitions of the experiment. The column "Dur." represents duration of the Phase I (time between execution of the first and the last request in the phase). Columns "CPU" and "Exec. time" represent average CPU usage and average execution time respectively. The row "Change" represents percentage difference between samples and is calculated using the formula \eqref{eq:threads}.

\begin{equation} \label{eq:threads}
Change = \frac{ControlSample - AptsAdaptation}{ControlSample}\cdot 100\%
\end{equation}

The data presented in the Table \ref{table:threads:summary} are visualized in the Figures \ref{figure:threads:results:duration} (the first phase duration for the samples), \ref{figure:threads:results:executiontime} (average execution time for the samples in different phases) and \ref{figure:threads:results:cpu} (average CPU usage for the samples in different phases). All results from
performed experiments are available in the Appendix \ref{appendix:threadsevaluation}.

\begin{table}[!htb]
\begin{center}
\begin{tabular}{l|ccC{1.5cm}|cC{1.5cm}|cC{1.5cm}|}
                 &  \multicolumn{3}{c|}{\textbf{Phase I}} & \multicolumn{2}{c|}{\textbf{Phase II}} & \multicolumn{2}{c|}{\textbf{Total}} \\
                 &  \textbf{Dur. [s]} & \textbf{CPU} & \textbf{Exec. time [ms]}  &\textbf{CPU} & \textbf{Exec. time [ms]}  &\textbf{CPU} & \textbf{Exec. time [ms]}  \\ \hline
Control sample      & 44 & 28.7\% & 5.0 & 100\% & 27.6 & 63.3\% & 10.6 \\
Adapted sample & 32 & 41,8\% & 5.0 & 99.9\% & 20.1 & 68.7\% & 8.8\\ \hline
Change 	            & 27.3\% & -45.6\% & 0.0\% &  0.1\% & 27.2\% & -8.5\% & 17.0\%
\end{tabular}
\end{center}
\caption{\textit{Summary of the experiment for control and adapted sample}} \label{table:threads:summary}
\end{table}

\begin{figure}[!htb]
\centering
\begin{tikzpicture}
        \begin{axis}[
            symbolic x coords={Control, Adapted},
            xtick=data,
            bar width=1cm,
            x=3cm,
            ymin=0,
            enlarge x limits={abs=2cm},
            xlabel={Sample},
            ymajorgrids = true,
            ylabel={Duration [s]}
          ]
            \addplot[ybar,style={bblue,fill=bblue,mark=none}] coordinates {
                (Control, 44) (Adapted, 32)
            };
        \end{axis}
\end{tikzpicture}
\caption{\textit{Graph presenting duration of the first phase of the experiment for control and adapted sample}} \label{figure:threads:results:duration}
\end{figure}

\pgfplotstableread[row sep=\\,col sep=&]{
    category  & control& adapted  \\
    Phase I   & 5   & 5 \\
    Phase II  & 27.6   & 20.1  \\
    Total     & 10.6   & 8.8  \\
    }\mydata
\begin{figure}[!htb]
\centering
\begin{tikzpicture}
    \begin{axis}[
            ybar,
            bar width=.6cm,
            width=.8\textwidth,
            height=.6\textwidth,
            legend style={at={(0.5,-0.15)},
                anchor=north,legend columns=-1},
            symbolic x coords={Phase I,Phase II,Total},
            xtick=data,
            ymin=0,ymax=30,
            ylabel={Average execution time [ms]},
            enlarge x limits={abs=2cm},
            ymajorgrids = true,
            legend image code/.code={\draw[#1, draw=none] (0cm,-0.15cm) rectangle (0.5cm,0.2cm);},
        ]
        \addplot[style={bblue,fill=bblue,mark=none}] table[x=category,y=control]{\mydata};
        \addplot[style={ggreen,fill=ggreen,mark=none}] table[x=category,y=adapted]{\mydata};
        \legend{Control sample, APTS adapted sample}
    \end{axis}
\end{tikzpicture}
\caption{\textit{Average execution time in a particular phases during the experiment for control and APTS adapted sample}} \label{figure:threads:results:executiontime}
\end{figure}

\pgfplotstableread[row sep=\\,col sep=&]{
    category  & control& adapted  \\
    Phase I   & 28.7   & 41.8 \\
    Phase II  & 100   & 99.9  \\
    Total     & 63.3   & 68.7  \\
    }\mydata
\begin{figure}[!htb]
\centering
\begin{tikzpicture}
    \begin{axis}[
            ybar,
            bar width=.6cm,
            width=.8\textwidth,
            height=.6\textwidth,
            legend style={at={(0.5,-0.15)}, anchor=north,legend columns=-1},
            symbolic x coords={Phase I,Phase II,Total},
            xtick=data,
            ymin=0,ymax=100,
            ylabel={Average CPU usage [\%]},
            enlarge x limits={abs=2cm},
            ymajorgrids = true,
            legend image code/.code={\draw[#1, draw=none] (0cm,-0.15cm) rectangle (0.5cm,0.2cm);}, 
        ]
        \addplot[style={oorange,fill=oorange,mark=none}] table[x=category,y=control]{\mydata};
        \addplot[style={rred,fill=rred,mark=none}] table[x=category,y=adapted]{\mydata};
        \legend{Control sample, APTS adapted sample}
    \end{axis}
\end{tikzpicture}
\caption{\textit{Average CPU usage in a particular phases during the experiment for control and APTS adapted sample}} \label{figure:threads:results:cpu}
\end{figure}


\subsubsection{Interpretation} 

The results show that duration of the first phase for the adapted sample was shorter by 27\%. Such improvement was achieved by the increase of the threads number, performed by the APTS during the first phase of the experiment. Increased number of threads increased CPU usage during the first phase. The character of the generated traffic caused that average execution time for this phase was equal for both samples. 

During the second phase of the experiment, CPU consuming requests were generated. Due to that fact CPU usage for the both samples was equal to 100\%. The average execution time of requests during this phase was faster for the adapted solution by 27.2\%. 

During the whole experiment adapted sample was able to achieve 17\% improvement of the average execution time of requests at the expense of increased (by 8.5\%) average CPU consumption. 

\subsubsection{Experiment Conclusions} 

The experiments prove that threads management provided by the APTS is able to decrease execution time of the incoming request into the tested application. The application under the APTS performance tuning was able to handle incoming requests in more optimal way during the first phase of the experiment. In case of the CPU consuming requests generated during the second phase, the system optimized the threads number in order to reduce execution time of requests. 

The experiment performed in this chapter is an example of low level components optimization - in this case configuration of the threads number on the application server. There exist many other parameters offered by the application servers influencing performance - for example: pool size, heap size, garbage collector algorithm etc. All of them have influence on the performance but in order to provide the best possible performance they should be adjusted to the current state (traffic characteristic, available resources) of the environment. 


\subsection{APTS Combined Evaluation}

In the previous experiments the APTS was evaluated in isolation for particular tunable elements. In this experiment all three tunable elements (cache, batch, threads) are combined in order to prove that system is able to optimize performance  of the tested application by management of different tunable elements.   

\subsubsection{Traffic Simulation Scenario} 

For the combined APTS evaluation, the traffic simulation presented in the Figure \ref{figure:trafficcombined} was implemented. 

\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}
\draw[->] (0,0) -- (10,0);
\foreach \x in {1,5,9}
\draw(\x cm,3pt) -- (\x cm, -3pt);

\draw (1,0) node[below=3pt] {$0$};
\draw (3,0) node[below=3pt] {$30$};
\draw (5,0) node[below=3pt] {$60$};
\draw (9,0) node[below=3pt] {$120$};

\draw[fill=ggreen] (5,0) rectangle (9,1);
\draw (7,0) node[above=6pt, align=center] {Wait requests};

\draw[fill=rred] (1,0) rectangle (3,1);
\draw (2,0) node[above=-2pt, align=center, white] {Intensive \\ inserts};

\draw (10.8,0) node {$time [s]$};
\end{tikzpicture}
\caption{\textit{Traffic simulation scenario for the combined evaluation}} \label{figure:trafficcombined}
\end{center}
\end{figure}

The simulation takes 120 seconds and is divided into two phases. In the first phase "Intensive inserts" 300 insert requests are generated with constant injection step (10 requests per second). The requests in this phase are generated by 30 seconds. During the second phase ("Wait requests"), which starts at 60th second of simulation, 60 "Execute wait task" requests are generated with constant injection step (1 request per second). The \textit{sleepTime} parameter for the "Execute wait task" requests is set to 5000 (5 seconds).

From the high level perspective, during the first phase, intensive inputs have three results - increased CPU usage, traffic is interpreted by the system as mutable and inserts level is recognized by the system as high.  High CPU usage should be handled by the APTS by switching threads number to lower level. Mutable traffic should result in disabling cache during this phase. High inserts level should be handled by the system by enabling batch mechanism. During the second phase, long tasks with low CPU usage changes the character of the traffic to immutable and inserts level to low. Low CPU usage with simultaneous long execution time should by handled by the system by increasing the threads number. Immutable traffic should lead the system to enabled cache. Because of low inserts level, the system should disable batch mechanism. 

\subsubsection{Evaluation Samples} 

The combined experiment is developed on two samples:
\begin{itemize}
\item Control sample - in which performance tuning is disabled and configuration of the tested application is set to default (see Table \ref{table:initconfiguration}). 
\item Adapted Sample - in which performance tuning by the APTS is enabled and the initial
configuration is default (see Table 17). 
\end{itemize}

\subsubsection{Experiment} 

The experiment was repeated 10 times for both control and adapted samples what in total gives 20 repetition of the experiment. In the Figure \ref{figure:combined:screen:control} and \ref{figure:combined:screen:adapted} screenshots from Grafana platform with experiment run for both samples is presented.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{combinedCtrl}
\caption{\textit{Control sample experiment run - Grafana screenshot}} \label{figure:combined:screen:control}
\end{figure}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{combinedEval}
\caption{\textit{APTS adapted sample experiment run - Grafana screenshot}} \label{figure:combined:screen:adapted}
\end{figure}

In the graphs, CPU consumption and requests execution time is presented. The experiment run was significantly different for both samples.

At the beginning of the experiment, execution time of the particular request was increasing due to high CPU usage. After few requests, execution time stabilizes on interval 1-5 seconds in case of control sample. In case of adapted sample execution time increases nearly to 10 seconds because of overhead connected with switching threads number. After few seconds execution time of requests decreases to nearly 1 second because of enabled batch. 

During the second phase, 5 seconds-long requests with small CPU usage are generated. During experiment performed on the control sample each request was executed in nearly 5 seconds. In case of adapted sample, the execution time for requests in this phase was minimized by enabling the cache mechanism. Moreover, the number of threads was increased, what results in increased CPU usage for this phase comparing it to the control sample.   

The performance improvements chosen by the APTS during the experiment on the adapted sample are visible in the system logs: 

\vspace{1mm}\noindent\fbox{\parbox{\textwidth}{\texttt{\footnotesize
21:20:21.945 (...) Chosen rule: SwitchTo10ThreadsRule \\
action: threads switched to 10 \\
21:20:40.890 (...) Chosen rule: EnableBatchRule\\
action: enable batch\\
21:21:20.995 (...) Chosen rule: EnableCacheRule\\
action: enable cache\\
21:21:21.324 (...) Chosen rule: SwitchTo20ThreadsRule\\
action: threads switched to 20\\
21:21:21.845 (...) Chosen rule: DisableBatchRule\\
action: disable batch\\
21:21:30.560 (...) Chosen rule: SwitchTo30ThreadsRule\\
action: threads switched to 30
}}}\vspace{1mm}

\subsubsection{Results} 

The summary of performed experiment is presented in the Table \ref{table:combined:results:summary}. The summary presents average values computed from 10 repetitions of the experiment. The row "Change" represents percentage difference between samples and is calculated using the formula \eqref{eq:combined}.

\begin{equation} \label{eq:combined}
Change = \frac{ControlSample - AptsAdaptation}{ControlSample}\cdot 100\%
\end{equation}

The data presented in the Table \ref{table:combined:results:summary} are visualized in the Figures \ref{figure:combined:results:executiontime} and \ref{figure:combined:results:cpuusage}. All results from performed experiments are available in the Appendix \ref{appendix:combinedevaluation}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{l|c|c}
    & \textbf{Average execution time [ms]} & \textbf{Average CPU usage} \\ \hline
    Control	sample	  &3.27     & 47.26\% \\ 
    Adapted sample	  &2.02	    & 49.22\% \\ \hline
    Change		      &38.23\%	& -4.15\%
\end{tabular}
\end{center}
\caption{System evaluation measurements - combined solution measurements summary} \label{table:combined:results:summary}
\end{table}

\begin{figure}[!htb]
\centering
\begin{tikzpicture}
        \begin{axis}[
            symbolic x coords={Control, Adapted},
            xtick=data,
            bar width=1cm,
            x=3cm,
            ymin=0,
            enlarge x limits={abs=2cm},
            xlabel={Sample},
            ymajorgrids = true,
            ylabel={Execution time [s]},
          ]
            \addplot[ybar,style={ggreen,fill=ggreen,mark=none}] coordinates {
                (Control, 3.27) (Adapted, 2.02)
            };
        \end{axis}
\end{tikzpicture}
\caption{\textit{Graph presenting comparison of mean execution time between control and adapted sample}} \label{figure:combined:results:executiontime}
\end{figure}

\begin{figure}[!htb]
\centering
\begin{tikzpicture}
        \begin{axis}[
            symbolic x coords={Control, Adapted},
            xtick=data,
            bar width=1cm,
            x=3cm,
            ymin=0,
            enlarge x limits={abs=2cm},
            xlabel={Sample},
            ymajorgrids = true,
            ylabel={Average CPU usage [\%]}
          ]
            \addplot[ybar,style={oorange,fill=oorange,mark=none}] coordinates {
                (Control, 47.26) (Adapted, 49.22)
            };
        \end{axis}
\end{tikzpicture}
\caption{\textit{Graph presenting comparison of mean CPU usage between control and adapted sample}} \label{figure:combined:results:cpuusage}
\end{figure}


\subsubsection{Interpretation} 

The experiment results show that the APTS was able to reduce mean execution time of requests by 38.23\%. The improvement was achieved at the expense of relatively small increase of CPU usage. 

The execution time in the first phase was reduced by combined batch and threads number management provided by the APTS. During the seconds phase, execution time was reduced by optimization based on cache and threads number. Increased CPU usage can be recognized during the second phase of the experiment, when more threads were introduced. The cost of increased threads number is always higher CPU usage and in case of this experiment was acceptable.  

\subsubsection{Experiment Conclusions} 

The main goal of this experiment was to prove that the Automated Performance Tuning System works as a whole. The system was able to optimize tested application configuration to increase performance. The performance issues were correctly recognized and actions performed by the implemented rules led to increased performance of the tested application. Finally, aggregated improvement introduced by all three tunable elements (cache, batch, threads) was equal to nearly 40\%.

Apart from the achieved performance improvement, the experiment evaluated the APTS with all enabled features. The performed experiment is an example of combining different parts of performance optimization together. 


\subsection{APTS Evaluation on Random Samples}

\subsubsection{Motivation}
In the previous experiments presented in this chapter, the traffic simulation scenarios were created for the need of check if the system behaves properly under given circumstances. Due to that fact the simulation scenario may be considered as artificial and too sterile. Because of that the system should be also tested in under more real life scenario. 

The best possible option would be to ask real users to use the tested application. Unfortunately, limited practical applications of the tested application and lack of graphical user interface makes this idea hard to realize. 

Another idea is to generate traffic scenario on the base of a real traffic statistics of an existing web application related to similar domain. Unfortunately, author of this work does not have access to such application where such statistics could be gathered. A publicly available statistics from a web sites do not contain information about HTTP method type (which is necessary to evaluate the system) and are too high level oriented (provides information about number of visitors on given page or users characteristics - browsers, devices etc.).  

The last idea in order to create more natural traffic simulation scenario is to generate random data. In this solution randomly generation of traffic can create traffic which will be close to real one. For the need of this experiment ten traffic simulation scenarios will be randomly generated. In the Chapter \ref{section:randomgeneration} details about generated simulations are introduced. 

\subsubsection{Traffic Simulation Scenario Random Generation} \label{section:randomgeneration}

In order to evaluate the system in long time perspective generated traffic scenarios are 10 minutes long. 
During each simulation five different types of request are be used:
\begin{itemize}
\item Find product category by ID (find)
\item Update product	category (update)
\item Add new product category (add)
\item Execute CPU consuming task (cpuTask)
\item Execute wait task (wait)
\end{itemize}

The random traffic for each request type is generated on the base of 4 parameters:
\begin{itemize}
\item[\textit{Rate1}] - lower bound (inclusive) of interval from which random number of user per second is generated
\item[\textit{Rate2}] - upper bound (inclusive) of interval from which random number of user per second is generated  (must be greater than Rate 1)
\item[\textit{StartAt}] - number of seconds after which traffic starts to be generated
\item[\textit{Duration}] - number of seconds defining for how log the traffic will be generated (the \textit{Duration} summed with \textit{Starts At} must be not greater than 10 minutes - the duration of whole experiment)
\end{itemize}

For example, the following values of parameters:

\begin{center}
\begin{itemize}
\item[\textit{Rate1}] = 10
\item[\textit{Rate2}] = 20
\item[\textit{StartAt}] = 60
\item[\textit{Duration}] = 300
\end{itemize}
\end{center}

mean that traffic will start to be generated in 60th second of the experiment. After 5 minutes (300 seconds), that is at sixth minute of the experiment, the traffic generation finishes. During the generation the number of generated requests is random in every second. The possible value of generated requests in a given second is not less that 10 and not greater than 20 requests. 

Because of limited resources of the evaluation environment the following limits for random values of the \textit{Rate 1} and \textit{Rate 2} are introduced during generation.  For each request type, except \textit{Execute CPU consuming task}, the limits are set to the interval $[0,6]$. Because of high CPU consumption by the \textit{Execute CPU consuming task} requests, the limit for that requests type is set to the interval $[0,1]$. 

The Table \ref{table:random:traffic:possiblevalues} is concluding above information. The table shows possible interval of parameters values for the given request type. For more readability an additional column - \textit{EndAt} - is introduced (the column informs at which second of the experiment the traffic generation will stop). 

\newcommand{\twodots}{\mathinner {\ldotp \ldotp}}

\begin{table}[ht]

\begin{center}
\begin{tabular}{c|c|c|c|c|c}
\textbf{Request type}& \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration} \\\hline
find	&$[0 \twodots 6]$    &$[\textit{Rate1} \twodots 6]$    &\multirow{5}{*}{$[0 \twodots 600]$}&\multirow{5}{*}{$[\textit{StartAt} \twodots 600]$}&\multirow{5}{*}{$[0 \twodots 600 -\textit{StartAt}]$}\\ \cline{1-3}
update	&$[0 \twodots 6]$    &$[\textit{Rate1} \twodots 6]$	  &	                     & 	                    \\ \cline{1-3}
insert	&$[0 \twodots 6]$    &$[\textit{Rate1} \twodots 6]$	  &	                     &                      \\ \cline{1-3}
wait	&$[0 \twodots 6]$    &$[\textit{Rate1} \twodots 6]$	  &	                     &                      \\ \cline{1-3}
cpuTask	&$[0 \twodots 1]$     &$[\textit{Rate1} \twodots 1]$     &	                     & 	                   \\  
\end{tabular}
\end{center}
\caption{\textit{Possible values of randomly generated traffic parameters}} \label{table:random:traffic:possiblevalues}
\end{table}


\subsubsection{Evaluation Samples}

For each generated traffic scenario simulation two experiments are run  - one for control sample and second for adapted sample. The used  samples are described as follows:
\begin{itemize}
\item Control sample - sample in which performance tuning will be disabled and configuration of the tested application will be set to default (see Table \ref{table:initconfiguration}). 
\item Adapted Sample - sample in which performance tuning by the APTS is enabled and the initial configuration is default (see Table \ref{table:initconfiguration}). 
\end{itemize}




\subsubsection{Experiment Run - 1st Repetition}

This section presents the first repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:1}. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&1	&4	&582	&585	&3     \\ \hline
update	&4	&4	&54	    &584	&530   \\ \hline
insert	&3	&3	&471	&535	&64    \\ \hline
wait	&2	&5	&300	&401	&101   \\ \hline
cpuTask	&1	&1	&171	&374	&203   \\  
\end{tabular}
\end{center}
\caption{\textit{First repetition of the experiment - generated traffic}} \label{table:random:traffic:1}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:1} and \ref{figure:random:screen:adapted:1} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1-ctrl}
\caption{\textit{First repetition of the experiment - control sample}} \label{figure:random:screen:control:1}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1-adap}
\caption{\textit{First repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:1}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:1} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 
\begin{enumerate}
\item 18:03:08.251 - Configuration of CACHE changed to value: CACHED
\item 18:04:08.325 - Configuration of CACHE changed to value: NO\_CACHE
\item 18:06:27.973 - Configuration of THREADS changed to value: T20
\item 18:06:58.489 - Configuration of CACHE changed to value: CACHED
\item 18:07:07.756 - Configuration of CACHE changed to value: NO\_CACHE
\item 18:08:37.782 - Configuration of CACHE changed to value: CACHED
\item 18:08:48.175 - Configuration of CACHE changed to value: NO\_CACHE
\item 18:18:48.072 - Configuration of THREADS changed to value: T20
\item 18:18:58.217 - Configuration of THREADS changed to value: T30
\item 18:19:48.526 - Configuration of THREADS changed to value: T20
\end{enumerate}







\subsubsection{Experiment Run - 2nd Repetition}

This section presents second repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:2}. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&2	&3	&353	&417	&64\\ \hline
update	&1	&3	&51	    &149	&98\\ \hline
insert	&1	&5	&91	    &259	&168\\ \hline
wait	&5	&5	&391	&484	&93\\ \hline
cpuTask	&1	&1	&385	&413	&28
\end{tabular}
\end{center}
\caption{\textit{Second repetition of the experiment - generated traffic}} \label{table:random:traffic:2}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:2} and \ref{figure:random:screen:adapted:2} respectively. 


\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{2-ctrl}
\caption{\textit{Second repetition of the experiment - control sample}} \label{figure:random:screen:control:2}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{2-adap}
\caption{\textit{Second repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:2}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:2} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 18:47:54.562 - Configuration of CACHE changed to value: CACHED
\item 18:48:44.512 - Configuration of CACHE changed to value: NO\_CACHE
\item 18:50:14.498 - Configuration of THREADS changed to value: T10
\item 18:52:24.394 - Configuration of CACHE changed to value: CACHED
\end{enumerate}






\subsubsection{Experiment Run - 3rd Repetition}

This section presents third repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:3}. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&5	&6	&181	&597	&416\\ \hline
update	&5	&5	&26	    &100	&74\\ \hline
insert	&5	&5	&490	&550	&60\\ \hline
wait	&4	&5	&495	&505	&10\\ \hline
cpuTask	&0	&0	&405	&442	&37
\end{tabular}
\end{center}
\caption{\textit{Third repetition of the experiment - generated traffic}} \label{table:random:traffic:3}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:3} and \ref{figure:random:screen:adapted:3} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{3-ctrl}
\caption{\textit{Third repetition of the experiment - control sample}} \label{figure:random:screen:control:3}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{3-adap}
\caption{\textit{Third repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:3}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:3} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 21:03:32.846 - Configuration of CACHE changed to value: CACHED
\item 21:04:02.562 - Configuration of CACHE changed to value: NO\_CACHE
\item 21:05:52.467 - Configuration of CACHE changed to value: CACHED
\item 21:11:42.637 - Configuration of CACHE changed to value: NO\_CACHE
\item 21:11:52.860 - Configuration of THREADS changed to value: T10
\item 21:14:32.410 - Configuration of CACHE changed to value: CACHED
\end{enumerate}






\subsubsection{Experiment Run - 4th Repetition}

This section presents fourth repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:4}. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&1	&2	&58	    &240	&182\\ \hline
update	&1	&6	&272	&351	&79\\ \hline
insert	&1	&4	&472	&472	&0\\ \hline
wait	&0	&3	&165	&353	&188\\ \hline
cpuTask	&1	&1	&19	    &529	&510
\end{tabular}
\end{center}
\caption{\textit{Fourth repetition of the experiment - generated traffic}} \label{table:random:traffic:4}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:3} and \ref{figure:random:screen:adapted:4} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{4-ctrl}
\caption{\textit{Fourth repetition of the experiment - control sample}} \label{figure:random:screen:control:4}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{4-adap}
\caption{\textit{Fourth repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:4}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:4} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 21:44:23.035 - Configuration of CACHE changed to value: CACHED
\item 21:44:42.660 - Configuration of THREADS changed to value: T30
\item 21:44:53.154 - Configuration of THREADS changed to value: T20
\item 21:45:12.940 - Configuration of THREADS changed to value: T30
\item 21:48:52.571 - Configuration of CACHE changed to value: NO\_CACHE
\item 21:49:02.861 - Configuration of THREADS changed to value: T20
\item 21:49:13.074 - Configuration of THREADS changed to value: T10
\item 21:53:32.624 - Configuration of CACHE changed to value: CACHED
\end{enumerate}






\subsubsection{Experiment Run - 5th Repetition}

This section presents fifth repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:5}. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&2	&3	&183	&421	&238\\ \hline
update	&0	&3	&451	&480	&29\\ \hline
insert	&2	&5	&62	    &333	&271\\ \hline
wait	&5	&6	&257	&467	&210\\ \hline
cpuTask	&1	&1	&234	&260	&26
\end{tabular}
\end{center}
\caption{\textit{Fifth repetition of the experiment - generated traffic}} \label{table:random:traffic:5}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:5} and \ref{figure:random:screen:adapted:5} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{5-ctrl}
\caption{\textit{Fifth repetition of the experiment - control sample}} \label{figure:random:screen:control:5}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{5-adap}
\caption{\textit{Fifth repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:5}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:5} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 22:28:53.174 - Configuration of CACHE changed to value: CACHED
\item 22:30:02.620 - Configuration of CACHE changed to value: NO\_CACHE
\item 22:32:42.963 - Configuration of THREADS changed to value: T10
\item 22:35:03.595 - Configuration of THREADS changed to value: T20
\item 22:35:12.719 - Configuration of THREADS changed to value: T10
\item 22:35:33.142 - Configuration of THREADS changed to value: T20
\item 22:35:43.398 - Configuration of THREADS changed to value: T10
\item 22:36:33.374 - Configuration of THREADS changed to value: T20
\item 22:36:43.618 - Configuration of THREADS changed to value: T10
\item 22:37:03.045 - Configuration of THREADS changed to value: T20
\item 22:37:13.248 - Configuration of THREADS changed to value: T30
\item 22:38:52.868 - Configuration of CACHE changed to value: CACHED
\item 22:39:43.434 - Configuration of CACHE changed to value: NO\_CACHE
\end{enumerate}






\subsubsection{Experiment Run - 6th Repetition}

This section presents sixth repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:6}. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&0	&0	&37	    &193	&156 \\ \hline
update	&6	&6	&565	&593	&28\\ \hline
insert	&6	&6	&419	&427	&8\\ \hline
wait	&5	&5	&448	&493	&45\\ \hline
cpuTask	&0	&1	&167	&430	&263\\ \hline
\end{tabular}
\end{center}
\caption{\textit{Sixth repetition of the experiment - generated traffic}} \label{table:random:traffic:6}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:6} and \ref{figure:random:screen:adapted:6} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{6-ctrl}
\caption{\textit{Sixth repetition of the experiment - control sample}} \label{figure:random:screen:control:6}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{6-adap}
\caption{\textit{Sixth repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:6}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:6} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 23:00:00.355 - Configuration of CACHE changed to value: CACHED
\item 23:07:00.187 - Configuration of CACHE changed to value: NO\_CACHE
\item 23:07:20.352 - Configuration of CACHE changed to value: CACHED
\item 23:07:20.519 - Configuration of THREADS changed to value: T10
\item 23:07:30.689 - Configuration of THREADS changed to value: T20
\item 23:07:40.805 - Configuration of THREADS changed to value: T30
\item 23:09:29.889 - Configuration of CACHE changed to value: NO\_CACHE
\item 23:09:40.059 - Configuration of THREADS changed to value: T20
\end{enumerate}






\subsubsection{Experiment Run - 7th Repetition}

This section presents seventh repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:7}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&5	&6	&93	    &437	&344\\\hline
update	&2	&3	&457	&478	&21\\\hline
insert	&0	&6	&277	&314	&37\\\hline
wait	&4	&5	&36	    &304	&268\\\hline
cpuTask	&0	&0	&292	&552	&260\\\hline
\end{tabular}
\end{center}
\caption{\textit{Seventh repetition of the experiment - generated traffic}} \label{table:random:traffic:7}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:7} and \ref{figure:random:screen:adapted:7} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{7-ctrl}
\caption{\textit{Seventh repetition of the experiment - control sample}} \label{figure:random:screen:control:7}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{7-adap}
\caption{\textit{Seventh repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:7}
\end{figure}


Additionally, in the Figure \ref{figure:random:screen:adapted:7} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 23:37:12.884 - Configuration of CACHE changed to value: CACHED
\item 23:41:52.336 - Configuration of CACHE changed to value: NO\_CACHE
\item 23:42:02.591 - Configuration of THREADS changed to value: T10
\item 23:42:33.177 - Configuration of THREADS changed to value: T20
\item 23:42:42.418 - Configuration of THREADS changed to value: T10
\item 23:43:33.098 - Configuration of CACHE changed to value: CACHED
\item 23:44:53.033 - Configuration of CACHE changed to value: NO\_CACHE
\end{enumerate}







\subsubsection{Experiment Run - 8th Repetition}

This section presents eighth repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:8}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&5	&6	&207	&467	&260\\\hline
update	&2	&2	&549	&549	&0\\\hline
insert	&6	&6	&598	&598	&0\\\hline
wait	&6	&6	&43	    &296	&253\\\hline
cpuTask	&1	&1	&510	&545	&35
\end{tabular}
\end{center}
\caption{\textit{Eighth repetition of the experiment - generated traffic}} \label{table:random:traffic:8}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:8} and \ref{figure:random:screen:adapted:8} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{8-ctrl}
\caption{\textit{Eighth repetition of the experiment - control sample}} \label{figure:random:screen:control:8}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{8-adap}
\caption{\textit{Eighth repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:8}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:8} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 00:05:35.937 - Configuration of CACHE changed to value: CACHED
\item 00:09:26.191 - Configuration of THREADS changed to value: T10
\end{enumerate}






\subsubsection{Experiment Run - 9th Repetition}

This section presents ninth repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:9}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&3	&4	&420	&465	&45\\\hline
update	&3	&4	&294	&413	&119\\\hline
insert	&4	&6	&349	&519	&170\\\hline
wait	&4	&5	&73  	&593	&520\\\hline
cpuTask	&0	&1	&150	&346	&196
\end{tabular}
\end{center}
\caption{\textit{Nineth repetition of the experiment - generated traffic}} \label{table:random:traffic:9}
\end{table}


The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:9} and \ref{figure:random:screen:adapted:9} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{9-ctrl}
\caption{\textit{Nineth repetition of the experiment - control sample}} \label{figure:random:screen:control:9}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{9-adap}
\caption{\textit{Nineth repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:9}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:9} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 11:37:28.648 - Configuration of CACHE changed to value: CACHED
\item 11:42:28.943 - Configuration of CACHE changed to value: NO\_CACHE
\item 11:42:38.261 - Configuration of THREADS changed to value: T10
\item 11:46:18.651 - Configuration of THREADS changed to value: T20
\item 11:46:28.919 - Configuration of THREADS changed to value: T10
\item 11:47:19.035 - Configuration of THREADS changed to value: T20
\item 11:47:28.138 - Configuration of THREADS changed to value: T30
\item 11:53:28.517 - Configuration of CACHE changed to value: CACHED
\end{enumerate}





\subsubsection{Experiment Run - 10th Repetition}

This section presents tenth repetition of the experiment using randomly generated traffic simulation scenario. The generated parameters (described in the Chapter \ref{section:randomgeneration}) used in the created scenario are presented in the Table \ref{table:random:traffic:10}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}}
\textbf{Request type} & \textbf{Rate1} & \textbf{Rate2} & \textbf{StartAt} & \textbf{EndAt} & \textbf{Duration}\\\hline
find	&5	&6	&156	&405	&249\\\hline
update	&4	&6	&60	    &597	&537\\\hline
insert	&5	&5	&83	    &583	&500\\\hline
wait	&2	&5	&3	    &308	&305\\\hline
cpuTask	&0	&1	&184	&264	&80
\end{tabular}
\end{center}
\caption{\textit{Tenth repetition of the experiment - generated traffic}} \label{table:random:traffic:10}
\end{table}

The Grafana screens presenting the experiment run for the control sample and adapted sample are presented in the Figure \ref{figure:random:screen:control:10} and \ref{figure:random:screen:adapted:10} respectively. 

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{10-ctrl}
\caption{\textit{Tenth repetition of the experiment - control sample}} \label{figure:random:screen:control:10}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{10-adap}
\caption{\textit{Tenth repetition of the experiment - adapted sample}} \label{figure:random:screen:adapted:10}
\end{figure}

Additionally, in the Figure \ref{figure:random:screen:adapted:10} configuration changes made by the APTS during the experiment made on adapted sample are marked as numbers at the top of the graph. The configuration change assigned to each number is explained in below listing: 

\begin{enumerate}
\item 12:28:15.072 - Configuration of CACHE changed to value: CACHED
\item 12:30:19.252 - Configuration of CACHE changed to value: NO\_CACHE
\item 12:30:28.498 - Configuration of THREADS changed to value: T10
\item 12:44:08.624 - Configuration of THREADS changed to value: T20
\item 12:44:28.964 - Configuration of THREADS changed to value: T10
\item 12:45:48.357 - Configuration of THREADS changed to value: T20
\item 12:45:58.598 - Configuration of THREADS changed to value: T10
\end{enumerate}

\subsubsection{Results} 

The result summary of the performed experiments on 10 randomly generated scenarios is presented in the Table \ref{table:random:resultssummary}. The summary presents average request execution time and average CPU usage during whole experiment performed on particular generated scenario. The columns "Execution Time Change" and "Average CPU Usage Change" represent percentage difference between control and adapted sample and are calculated using the formula \eqref{eq:random}.

\begin{equation} \label{eq:random}
Change=\frac{controlSample-adaptedSample}{controlSample} \cdot 100\%
\end{equation}

{\renewcommand{\arraystretch}{1.2}
\begin{table}[ht]
\begin{center}
\begin{tabular}{c c | C{3cm} | C{3cm} | C{3cm} | C{3cm}}
                 \textbf{\#} & \textbf{Sample} &  \textbf{Total Execution Time [s]} & \textbf{Average CPU Usage} & \textbf{Execution Time Change} & \textbf{Average CPU Usage Change} \\ \hline 
 \multirow{2}{*}{1} & Control&     4.00          &     92\%   & \multirow{2}{*}{14.29\%}&\multirow{2}{*}{-1.09\%} \\ 	  
                    & Adapted&     3.50          &     93\%   & \\ \hline
 \multirow{2}{*}{2} & Control&     2.80          &     43\%   & \multirow{2}{*}{428.30\%}&\multirow{2}{*}{-4.65\%}\\ 	  
                    & Adapted&     0.53          &     45\%   & \\ \hline 
 \multirow{2}{*}{3} & Control&     0.45          &     88\%   & \multirow{2}{*}{-15.09\%}&\multirow{2}{*}{21.59\%}\\ 	  
                    & Adapted&     0.53          &     69\%   & \\ \hline 
 \multirow{2}{*}{4} & Control&     16.00         &     96\%   & \multirow{2}{*}{370.59\%}&\multirow{2}{*}{20.83\%}\\ 	  
                    & Adapted&     3.40          &     76\%   & \\ \hline 
 \multirow{2}{*}{5} & Control&     3.40          &     67\%   & \multirow{2}{*}{78.95\%}&\multirow{2}{*}{-1.49\%}\\ 	  
                    & Adapted&     1.90          &     68\%   & \\ \hline
 \multirow{2}{*}{6} & Control&     6.60          &     50\%   & \multirow{2}{*}{650.00\%}&\multirow{2}{*}{62\%}\\ 	  
                    & Adapted&     0.88          &     19\%   & \\ \hline
 \multirow{2}{*}{7} & Control&     2.20          &     86\%   & \multirow{2}{*}{348,98\%}&\multirow{2}{*}{19.76\%}\\	  
                    & Adapted&     0.49          &     69\%   & \\ \hline 
 \multirow{2}{*}{8} & Control&     3.22          &     75\%   & \multirow{2}{*}{820\%}&\multirow{2}{*}{28\%}\\  
                    & Adapted&     0.35          &     54\%   & \\ \hline 
 \multirow{2}{*}{9} & Control&     4.20          &     72\%   & \multirow{2}{*}{110.00\%}&\multirow{2}{*}{20.83\%}\\  
                    & Adapted&     2.00          &     57\%   & \\ \hline 
 \multirow{2}{*}{10}& Control&     2.30          &     96\%   & \multirow{2}{*}{64.29\%}&\multirow{2}{*}{1.04\%}\\ 	  
                    & Adapted&     1.40          &     95\%   &\\   \hline   
                    & Average&                   &            & 287.03\%&16.68\%\\ 	      
\end{tabular}
\end{center}
\caption{Evaluation on randomly generated traffic scenarios - results summary}
\label{table:random:resultssummary}
\end{table}
}

More detailed data about performed experiments on randomly generated traffic scenarios is available in the Appendix \ref{appendix:random}.

\subsubsection{Interpretation}

Analysis of the experiment results from the Table \ref{table:random:resultssummary} shows that the APTS was able to improve performance of the tested application in most cases (9/10).  The only case finished with failure was repetition number 3. 

The reasons of the failure can be found in the experiment execution. Comparison of both - the control and the adapted - executions clearly show that sample adapted by the APTS has greater requests execution time during "wait" and "add" requests execution time. The reason of the increased execution time for these two request types is changed threads number configuration - from 20 to 10 threads. During such change new requests are forwarded to server instance configured with 10 threads. At the same time "old" requests have to be finished at the server instance configured with 20 threads. Because of that CPU is even more loaded at the short moment after the configuration switch. Loaded CPU results in increased execution time of the requests. During the experiment, the APTS was not able to compensate this slowdown in further phase because requests of type "wait" and "add" were generated at the relatively short period of time. 

In some cases the adapted solution decreased requests execution time to the detriment of increased CPU usage. This can be interpreted also as optimization because the same amount of work was done in shorter time increasing CPU usage. The reason of situations in which both - execution time and CPU usage - were decreased by the APTS is usage of cache - instead of computing response the application returned it from the memory. 

The percentage difference between control and adapted samples presented in the column "Execution Time Change" in the Table \ref{table:random:resultssummary} represents wide range of values - from -15.09\% (no improvement) to 820\%. Such variance implicates the statement that effectiveness of the APTS depends on the character of incoming traffic.  


\subsubsection{Experiment Conclusions}

The main goal of this experiment was to check how the APTS will behave under more "real" web traffic. To simulate such traffic a method consisting of random traffic generation was used. The duration of the simulated scenario was extended to 10 minutes. The results of the experiment shows that the system was able to improve performance of the executed requests in the 9 of 10 executed cases. The level of the obtained improvement is varied and depends of the traffic against which optimization is performed.


\section{Conclusions} \label{section:conclusions}

\subsection{Addressing Formulated Problem}

This work is dedicated to subject of software optimization based on the performance evaluation tests. The work addresses the software optimization in the complex way - from theoretical literature overview for the domain of software performance and optimization through presentation of performance analysis of selected mechanisms to practical realization of software optimization in the form of proposed system - the Automatic Performance Tuning System. The concept of the performance evaluation tests is theoretically introduced. The goals, types, possible implementations and main features of the performance evaluation tests are presented. The practical usage of the performance evaluation tests is implemented in the APTS - the main part of the system is based on the implemented performance evaluation tests. The performance evaluation tests are used in the introduced system to provide performance monitoring and performance issues reporting.

The main goal, which was set starting this work was development of a solution which will be able to optimize web application by optimization of application configuration parameters and automatic adjustment.  The research questions formulated at the initial phase of this work where:
\begin{itemize}
\item Which and how application parameters influence performance?
\item How to evaluate performance of the system?
\item How performance evaluation tests can help in software optimization - in monitoring and application performance adjustment
\end{itemize} 

The realization of the main goal of the work is implemented Automated Performance Tuning System which is based on performance evaluation tests. Main features listed in the goal were achieved - The system is able to monitor the environment on which a tested application is deployed and configuration adjustment is performed in order to optimize the monitored application form the performance perspective. The effectiveness of the implemented system was confirmed in the set of performed experiments. During the experiments ability to optimize particular aspects of the application in isolation as well as together was proven. The experiments performed on randomly generated traffic scenarios confirmed that the system is able to optimize performance of the tested application in different traffic characteristic.

In order to answer the first research question - which and how application parameters influence performance - the analysis of the possible performance tuning aspects was performed. The analysis point out the following areas on which performance optimization can be performed: Java environment optimization, presentation layer optimization, application server configuration, redundant components switching at runtime, application algorithms  optimization (used mechanism provided by the Spring Framework). From the analyzed areas to further, experimental analysis, 4 elements were selected - batch, cache, paging (which touch areas of redundant components, application algorithms optimization) and threads number (from the area of application server configuration). The selection was based on expected performance improvement in the web environment and practical justification in the domain of the implemented solution. On the base of performed experiments and considerations, finally, three elements were selected as the most promising in the context of performance optimization in the system domain - cache, batch and threads number. The selected elements were implemented into the tested application and server environment as a configurable parameters. The influence of the selected elements on the performance was confirmed and evaluated on the base of performed experiments. 

The second question - how to evaluate performance of the system - was analyzed in the section with theoretical background. In the section the term KPI was introduced and the set of possible indicators was listed. On the base of the theoretical background the practical realization of the selected KPIs was introduced in the APTS. In the system response time (execution time of request), utilization (CPU usage), workload profile (inserts level and traffic profile) were used to evaluate performance of the system.

The last question - how performance evaluation tests can help in software optimization - in monitoring and application performance adjustment - is answered in this work by the practical example - the APTS implementation. In the system, performance evaluation tests are responsible for monitoring. The monitoring is the source of further optimization decision made by the decision module of the system.  


\subsection{About the Implemented System}

At this point author of this work wants to highlight that implemented system - the APTS - is only a proof of concept of the solution which enables automated performance optimization on the base of performed monitoring. The system realizes performance optimization on the selected set of implemented tunable elements which match to the implemented tested application domain (which has no high practical value). Also important is fact that selected tunable elements are not universal. The selection of the tunable elements under automatic optimization must be adjusted to the application which will be optimized (as it is well known - there is no two the same applications in the world). Implemented solution should be considered as general form or framework for similar approach. 

In the Chapter \ref{principles} general principles of the system were formulated. All of the principles were realized in the final implementation. The system performs live monitoring through the implemented performance evaluation tests and the APTS manager. The system is autonomic - the implemented system is able to realize goal of the performance optimization automatically and without human intervention on the base of rule based system defined in decision module. The system is able to perform configuration change during application runtime because of implemented architecture of the system, which allows change of the configuration during the runtime. This feature is realized by the \textit{Decision Module} (performs change) and the \textit{Configuration Service} (delivers configuration for the tested application and provides API for configuration change). The solution is able to work in the production environment because it is independent from the testing environment - the data traffic generator is developed as external, separated  component. The real traffic in production environment can be monitored in the same way without any modification or configuration of the APTS system. The system has minimal impact on the production environment because of introduced deployment approach. All the components responsible for performance monitoring and optimization are deployed outside the production environment. The only component which for obvious reasons must be deployed on the production environment, is component responsible for resource monitoring.  

Another fact to consider is dependency between the APTS and the tested application. In the implemented solution the APTS intervenes into the application in two areas - configuration change and monitoring. 

In order to perform performance optimization, the tested application provides service - the \textit{Configuration Service} - which stores information about currently used configuration and allows the APTS to modify configuration at runtime of the application. Due to that fact the \textit{Configuration Service} may be considered as additional overhead which must be implemented on the application side in order to enable optimization by the APTS. However, in most of well designed and implemented applications the configuration is hold in one place - a class, a external file or a service. In case of a external file as well as service, the configuration may be also modified during the runtime of an application. The equivalent of such solution is just the \textit{Configuration Service} which provides one place to store a configuration and provides easy way to configure it a the runtime.  

The second area which has to be implemented on the tested application side in order to allow performance adaptation by the APTS is execution time and resource consumption monitoring. Both features are implemented in the form of components which are not interfered directly into the tested application, but still, they are deployed on the application side and generate additional overhead. However, in some applications (especially in a big, enterprise solutions) such components are implemented as a "core" part of an application in order to provide administrators current state of an application. In such case, assuming that the APTS would use data provided by an application, the overhead does not exist. 



\subsection{Problems and Issues}

During the realization of this work the author encounter few problems which were not considered at the beginning of the work.

One of the problem is related to the threads number configuration on the application server on which the tested application is deployed. Because the tested application is implemented on the base of Spring Boot project, the application comes with embedded Tomcat Server which reduced deployment process to only one step - execution of JAR with the application. Unfortunately, the Tomcat server does not allow to change threads number during the runtime. The Spring Boot provides alternative configurations, which allow Boot JARs to be deployed on different application servers. The try with the Glassfish server was made but without success (possible bug on the Glassfish side: GLASSFISH-21265).  Because of that the approach with request dispatcher and three instances of application with different number of threads was used (see Chapter \ref{section:systemdeployment}). 

Another problem was related to the deployment of the ready system in order to evaluate it. The evaluation experiments could not be performed on the single machine - overhead of testing environment may distort the results, moreover such solution would not fit to the proposed deployment diagram of the system. In order to solve that issue virtual machines for the tested application and helper server were introduced (see Chapter \ref{section:systemdeployment}). Due to that, the tested application was isolated from the overhead introduced by the performance optimization by the APTS. 

\subsection{Future Works}

The proposed solution realizes performance tuning decision on the base of decision module. In current implementation, decisions are made on the base of the rule based engine. Such solution has main disadvantage - the desirable optimal configuration must by implemented manually in the form of rules. As presented in this work, even for 3 tunable elements the effort and complexity of the rules is quite high. In the ideal world the system should be able to create such rules by itself in "learning" process. 

One of the possible improvement to achieve that would be change in the decision module engine. Instead of the rule based engine some artificial intelligence (machine learning) solution could be used. A good AI candidate for such work is a neural network which is able to recognize hidden patterns in the provided data. The current state of the tested environment (mean execution time of requests, resource usage) could be used as input to the network. As an output the optimal configuration would be returned. The drawback of the solution based on the machine learning is learning process. The teaching data (inputs and expected outputs) should be somehow prepared for the given environment. Because during learning the generated outputs could be highly inappropriate, the teaching process could not be conducted on production environment.

Another extension, which could be considered, is optimization of application server parameters. In this work due to the application domain and used solution (Tomcat server) only one optimization on the application server level was performed - the application server threads number. The more advanced application servers (like JBoss or IBM Websphere) provides many configurable parameters influencing performance which could be changed during a runtime. Implementation of automated performance tuning system dedicated for the given application server is a step into the more general solution - such system could be used in every environment which uses given application server.  

\pagebreak
\clearpage
\section{References} \label{section:references}
\renewcommand\refname{}
\begin{thebibliography}{9}

\bibitem{artperformance}
Molyneaux I., 
\textit{The Art of Application Performance Testing}, 
Sebastopol USA, O’Reilly Media, 2015.

\bibitem{javaperformance}
Oaks S., 
\textit{Java Performance: The Definitive Guide}, 
Sebastopol USA, O’Reilly Media, 2014.

\bibitem{optimizationtheory}
Rao S. S., 
\textit{Engineering Optimization Theory and Practice}, 
New Jersey USA, John Wiley \& Sons, Inc., 2009.

\bibitem{lssrarticle}
Smaalders B., 
\textit{Performance Anti-Patterns},
Queue, New York USA, vol. 4, no. 1, 2006

\bibitem{architectingperformance}
Kumar Shivakumar S. K. , 
\textit{Architecting High Performing, Scalable and Available Enterprise Web Applications},
Waltham USA, Elsevier, 2015, 101-141.

\bibitem{idealvalues}
Patel C. , Gulati R., 
\textit{Identifying Ideal Values of Parameters for Software Performance Testing},
IEEE, 2015 International Conference on Computing, Communication and Security (ICCCS), 4-5 Dec. 2015

\bibitem{comparison}
Kaur M., Kumari R.,
\textit{Comparative Study of Automated Testing Tools: TestComplete and QuickTest Pro},
International Journal of Computer Applications (0975 – 8887) Volume 24 - No.1, June 2011

\bibitem{analysisofpet}
Sharmila S., Ramadevi E.,
\textit{Analysis of Performance Testing on Web Applications},
International Journal of Advanced Research in Computer and Communication Engineering Vol. 3, Issue 3, March 2014

\bibitem{petmethodsandtools}
Sarojadevi H.,
\textit{Performance Testing: Methodologies and Tools}
Journal of Information Engineering and Applications, Vol 1, No.5, 2011.

\bibitem{howlong}
Fui-hoon F.,
\textit{A study on tolerable waiting time: how long are Web users willing to wait?}, Behaviour \& Information Technology, 23:3, 153-163, February 2007.

\bibitem{howlong}
Dang Q., Ignat C., 
\textit{Performance Evaluation of Web Based Automation Testing Tools}, Behaviour \& Information Technology, 23:3, 153-163, February 2007.

\bibitem{automaiontools}
Angmo R., Sharma M., 
\textit{Performance evaluation of web based automation testing tools}, 
5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence), Noida, 2014, pp. 731-735.

\bibitem{autobugs}
Tsakiltsidis S., Miranskyy A., Mazzawi E., 
\textit{On Automatic Detection of Performance Bugs},
IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), Ottawa, ON, Canada, 2016, pp. 132-139.

\bibitem{arttest}
Myers G. J. , Sandler C.,  Badgett T.,
\textit{The art of software testing, Third Edition},
Hoboken, New Jersey, USA, John Wiley \& Sons, Inc., 2012

\bibitem{testfoundations}
Spillner A., Linz T., Schaefer H.,
\textit{Software testing foundations, Fourth Edition},
Sebastopol, California, USA, O‘Reilly Media, 2014

\bibitem{set}
Buxton J.N.,Randell  B., 
\textit{Software Engineering Techniques}, 
Rome, Italy, Report on a conference sponsored by the NATO Science Committee, p. 16., 1970

\bibitem{glassfishdoc}
\textit{GlassFish Server Open Source Edition, Performance Tuning Guide}, 
https://glassfish.java.net/docs/4.0/performance-tuning-guide.pdf,
Oracle, May 2013

\bibitem{deployerproblem}
Raghavachari M., Reimer D., Johnson R. D., 
\textit{The deployer's problem: configuring application servers for performance and reliability}, 
25th International Conference on Software Engineering, 2003. Proceedings.,2003, pp. 484-489.

\bibitem{invariantsworkloads}
Menascé D. A., Almeida V. A. F., Riedi R., Pelegrinelli F., Fonseca R., Meira W.,  
\textit{In Search of Invariants for E-Business Workloads} 
In Proceeding of Second ACM Conference on Electronic Commerce, Minneapolis, October 17-20, 2000. 

\bibitem{autotuning}
Zhang Y., Qu W., Liu A., 
\textit{Automatic Performance Tuning for J2EE Application Server Systems.}  Web Information Systems Engineering. WISE 2005. Lecture Notes in Computer Science, vol 3806. Springer, Berlin, Heidelberg

\bibitem{autoarch}
Kephart J. O., Chess D. M., 
\textit{The vision of autonomic computing}, in Computer, vol. 36, no. 1, pp. 41-50, Jan 2003.

\bibitem{autoframework}
Diaconescu A., Mos A., Murphy J., \textit{Automatic performance management in component based software systems}, International Conference on Autonomic Computing, 2004. Proceedings., 2004, pp. 214-221.

\bibitem{redundancycomponent}
Diaconescu A., \textit{A framework for using component redundancy for self-adapting and self-optimising component-based enterprise systems}, 2003, In Companion of the 18th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications (OOPSLA '03). ACM, New York, NY, USA, 390-391. 

\bibitem{springperformance}
Chakraborty, A., Ditt, J., Vukotic, A., Machacek, J., \textit{Pro Spring 2.5}, Berkeley, CA, Apress, 2008, 829-855.  

\bibitem{gatling}
Gatling Load and Performance testing - Open-source load and performance testing - \textit{http://gatling.io}, (access: 15.04.2017).

\bibitem{influxdb}
InfluxData (InfluxDB) - Open Source Time Series Database for Monitoring Metrics and Events - \textit{https://www.influxdata.com}, (access: 18.04.2017).


\bibitem{junit}
JUnit - \textit{http://junit.org}, (access: 20.04.2017).

\bibitem{springaop}
Aspect Oriented Programming with Spring - \\
\textit{https://docs.spring.io/spring/docs/current/spring-framework-reference/html/aop.html}, (access: 06.05.2017).

\bibitem{nop}
Woolf B., Martin R. C., Riehle D., Buschmann F., \textit{"Null Object"}, In Pattern Languages of Program Design 3. Addison-Wesley Longman Publishing Co., Boston, USA, 1997, 5–18. 

\bibitem{grafana}
Grafana - The open platform for beautiful analytics and monitoring \textit{https://grafana.com/}, (access: 10.05.2017).

\end{thebibliography}

\pagebreak
\clearpage
\section{Appendices} \label{section:appendices}

\appendix
\setcounter{table}{0}
\renewcommand{\thesubsection}{\Alph{subsection}}
\renewcommand{\thetable}{\Alph{subsection}.\numberwithin{table}{subsection}}

\subsection{Influence of database content on the experiments} \label{appendix:influencedb}

\begin{table}[ht]
\begin{center}
\begin{tabular}{c| C{1.5cm}  C{1.5cm}  C{1.5cm} | C{1.5cm} C{1.5cm} C{1.5cm} |c}
                  & \multicolumn{6}{c|}{Execution time [ms]} \\
                  & \multicolumn{3}{c|}{1 row} & \multicolumn{3}{c|}{10000 rows} \\ 
    Repetition \# & find & update & add & find & update & add\\ \cline{1-7}
    1		  &1140& 618  & 659 & 1107& 558 & 994 &\\ 
    2		  &511 & 325  &	679 & 457 & 361 & 538 &\\ 
    3		  &365 & 423  &	781 & 389 & 405 & 618 &\\ 
    4		  &487 & 362  & 649 & 559 & 457 & 606 &\\ 
    5		  &481 & 434  &	693 & 476 & 496 & 717 &\\
    6		  &379 & 441  &	730 & 326 & 391 & 695 &\\ 
    7		  &454 & 412  & 682 & 387 & 387 & 697 &\\
    8		  &584 & 450  & 659 & 623 & 454 & 710 &\\
    9		  &423 & 435  &	612 & 544 & 354 & 693 &\\ 
    10		  &329 & 382  &	623 & 301 & 390 & 544 &\\ \cline{1-8}
        	  &515 & 428  & 677 & 517 & 425 & 681 & \multicolumn{1}{c}{Average} \\ 
\end{tabular}
\end{center}
\caption{Request execution time measurements performed on different database content}
\label{appendix:table:influencedb}
\end{table}


\pagebreak
\subsection{System evaluation measurements - cache} \label{appendixcacheeval}

\begin{table}[ht]
\begin{center}
\begin{tabular}{c| ccc |c}
    & \multicolumn{3}{C{5cm}|}{Average execution time of requests [ms]} & \\ 
    Repetition \# & Cached & No Cache & Adapted &\\ \cline{1-4}
    1		  &22.00 & 21.50  & 20.20 &\\ 
    2		  &20.27 & 20.51  &	20.63 &\\ 
    3		  &21.70 & 22.23  &	19.50 &\\ 
    4		  &19.67 & 19.90  & 20.04 &\\ 
    5		  &18.99 & 20.02  &	18.94 &\\
    6		  &19.09 & 19.06  &	18.42 &\\ 
    7		  &18.53 & 19.88  & 18.60 &\\
    8		  &18.53 & 18.93  &	18.89 &\\
    9		  &19.09 & 18.88  &	18.95 &\\ 
    10		  &19.69 & 19.05  &	18.25 &\\ \cline{1-5}
        	  &19.75 & 19.84  & 19.08 & \multicolumn{1}{c}{Average} \\ 
\end{tabular}
\end{center}
\caption{System evaluation measurements - cache related redundant components}
\label{appendixmeasurementscache}
\end{table}


\pagebreak
\subsection{Overhead of Spring ConcurrentMapCache class} \label{appendixspringconcurentmap}

\begin{table}[ht]
\begin{center}
\begin{tabular}{c| C{3cm}  C{3cm} |c}
    & \multicolumn{2}{c|}{Method execution time [ms]} \\ 
    Repetition \# & Put & Clear  &\\ \cline{1-3}
    1		  &0.55 & 0.53  &\\ 
    2		  &0.59 & 0.62  &\\ 
    3		  &0.57 & 0.48  &\\ 
    4		  &0.56 & 0.52  &\\ 
    5		  &0.62 & 0.62  &\\
    6		  &0.56 & 0.49  &\\ 
    7		  &0.55 & 0.51  &\\
    8		  &0.91 & 0.65  &\\
    9		  &0.55 & 0.48  &\\ 
    10		  &0.62 & 0.64  &\\ \cline{1-4}
        	  &0.61 & 0.56  & Average \\ 
\end{tabular}
\end{center}
\begin{center}
\caption{Execution time of selected methods from \textit{ConcurrentMapCache}} 
\end{center}
\label{appendixmeasurementscache}
\end{table}

\pagebreak
\subsection{Cached and not cached requests execution time} \label{appendixcachenotcached}

\begin{table}[ht]
\begin{center}
\begin{tabular}{c| C{3cm}  C{3cm} |c}
    & \multicolumn{2}{c|}{Request execution time [ms]} \\ 
    Repetition \# & No Cache & Cached  &\\ \cline{1-3}
    1		  &56.27 & 33.25  &\\ 
    2		  &50.83 & 33.86  &\\ 
    3		  &61.59 & 43.07  &\\ 
    4		  &48.26 & 44.61  &\\ 
    5		  &45.05 & 37.25  &\\
    6		  &41.22 & 31.13  &\\ 
    7		  &46.52 & 35.76  &\\
    8		  &44.09 & 31.68  &\\
    9		  &39.46 & 27.60  &\\ 
    10		  &35.51 & 55.60  &\\ \cline{1-4}
        	  &46.88 & 37.38  & Average \\ 
\end{tabular}
\end{center}
\caption{Requests execution time with enabled and disabled cache} 
\label{appendixmeasurementscache}
\end{table}


\pagebreak
\subsection{System evaluation measurements - batch} \label{appendixbatchevaluation}

\begin{table}[ht]
\begin{center}
\begin{tabular}{c| C{1.1cm}  C{1.2cm}  C{1.1cm}|C{1.1cm} C{1.2cm} C{1.1cm} |c}
    & \multicolumn{6}{c|}{Execution time [ms]}  \\ 
    & \multicolumn{3}{c|}{Control sample} & \multicolumn{3}{c|}{Adapted sample}\\ 
    Repetition \# & Phase I & Phase II & Total & Phase I & Phase II & Total &\\ \cline{1-7}
    1		  &32.6  &34.0    &33.9   &33.2  &23.5   &24.3   &\\ 
    2		  &23.6  &31.7    &31.0   &24.6  &23.4   &23.5   &\\ 
    3		  &34.2  &26.7    &27.3   &28.8  &22.8   &23.3   &\\ 
    4		  &24.8  &38.9    &37.7   &31.0  &22.2   &22.9   &\\ 
    5		  &30.5  &29.8    &29.9   &43.0  &22.3   &24.0   &\\ 
    6		  &30.9  &33.4    &33.2   &24.6  &23.9   &24.0   &\\ 
    7		  &33.8  &27.7    &28.2   &35.0  &20.8   &22.0   &\\ 
    8		  &29.2  &32.3    &32.0   &28.9  &21.6   &22.2   &\\ 
    9		  &25.8  &32.6    &32.0   &30.2  &22.3   &23.0   &\\ 
    10		  &34.1  &28.2    &28.7   &29.2  &21.9   &22.5   &\\ \cline{1-8}
        	  &30.0  &31.5    &31.4   &30.9  &22.5   &23.2   & \multicolumn{1}{c}{Average} \\ 
\end{tabular}
\end{center}
\caption{Batch management - evaluation measurements of control and adapted sample}
\label{appendixmeasurementscache}
\end{table}


\pagebreak


\subsection{System evaluation measurements - threads} \label{appendix:threadsevaluation}
{\renewcommand{\arraystretch}{0.91}
\begin{table}[ht]
\begin{center}
 \label{appendix:table:threadscontrol}
\begin{tabular}{c| C{1cm} C{1.3cm} C{1.5cm}|C{1.3cm} C{1.5cm} | C{1.3cm} C{1.5cm} |C{1cm}}
    & \multicolumn{3}{c|}{Phase I} & \multicolumn{2}{c|}{Phase II} & \multicolumn{2}{c|}{Total}\\ 
    Repetition \# & Duration [s] &  Average CPU Usage & Average Execution Time [s] & Average CPU Usage & Average Execution Time [s] & Average CPU Usage & Average Execution Time [s]\\  \cline{1-8}
    1 & 44 & 38.8\% & 5.0 & 100.0\% & 29.7 & 70.2\% & 11.2 \\
	2 & 44 & 30.6\% & 5.0 & 100.0\% & 20.3 & 64.5\% & 10.6 \\ 
    3 & 44 & 30.6\% & 5.0 & 100.0\% & 27.1 & 64.1\% & 10.6 \\
	4 & 43 & 27.3\% & 5.0 & 100.0\% & 27.4 & 63.3\% & 10.6 \\
	5 & 43 & 29.4\% & 5.0 & 100.0\% & 27.0 & 63.7\% & 10.5 \\
	6 & 43 & 29.0\% & 5.0 & 100.0\% & 25.8 & 61.7\% & 10.2 \\
	7 & 44 & 27.7\% & 5.0 & 100.0\% & 27.4 & 63.7\% & 10.6 \\
	8 & 44 & 25.3\% & 5.0 & 100.0\% & 27.4 & 62.7\% & 10.6 \\
	9 & 43 & 24.3\% & 5.0 & 100.0\% & 28.7 & 68.3\% & 10.9 \\
	10& 44 & 23.5\% & 5.0 & 100.0\% & 27.8 & 70.9\% & 10.7 \\ \cline{1-9}
   	  & 44 & 28.7\% & 5.0 & 100.0\% & 27.6 & 63.3\% & 10.6 & \multicolumn{1}{c}{Average} \\ 
\end{tabular}
\end{center}
\caption{Threads management - evaluation measurements of control sample}
\label{appendixmeasurementscache}
\end{table}

\begin{table}[ht]
\begin{center}
\label{appendix:table:threadeval}
\begin{tabular}{c| C{1cm} C{1.3cm} C{1.5cm}|C{1.3cm} C{1.5cm} | C{1.3cm} C{1.5cm} |C{1cm}}
    & \multicolumn{3}{c|}{Phase I} & \multicolumn{2}{c|}{Phase II} & \multicolumn{2}{c|}{Total}\\ 
    Repetition \# & Duration [s] &  Average CPU Usage & Average Execution Time [s] & Average CPU Usage & Average Execution Time [s] & Average CPU Usage & Average Execution Time [s]\\  \cline{1-8}
    1 & 30 & 50.6\% & 5.0 & 99.8\% & 19.0 & 70.8\% & 8.5 \\
    2 & 37 & 34.8\% & 5.0 & 100.0\% & 20.8 & 72.6\% & 9.0\\
    3 & 29 & 37.2\% & 5.0 & 99.8\% & 21.6 & 65.7\% & 9.1 \\
    4 & 29 & 39.5\% & 5.0 & 100.0\% & 19.0 & 69.4\% & 8.5 \\
    5 & 30 & 47.8\% & 5.0 & 99.9\% & 21.2 & 71.5\% & 9.1 \\
    6 & 46 & 37.8\% & 5.0 & 100.0\% & 18.1 & 69.1\% & 8.3 \\
    7 & 31 & 53.6\% & 5.0 & 100.0\% & 20.0 & 70.8\% & 8.8 \\
    8 & 30 & 36.7\% & 5.0 & 100.0\% & 17.2 & 64.6\% & 8.1 \\
    9 & 30 & 41.2\% & 5.0 & 99.8\% & 21.5 & 65.9\% & 9.2 \\
    10 & 30 & 38.6\% & 5.0 & 100.0\% & 22.4 & 66.1\% & 9.4 \\ \cline{1-9}
   	   & 32 & 41.8\% & 5.0 & 100.0\% & 20.1 & 68.7\% & 8.8 & \multicolumn{1}{c}{Average} \\ 
\end{tabular}
\end{center}
\caption{Threads management - evaluation measurements of adapted sample}
\label{appendixmeasurementscache}
\end{table}
}



\pagebreak

\subsection{System evaluation measurements - combined} \label{appendix:combinedevaluation}

\begin{table}[ht]
\begin{center}
\begin{tabular}{C{1.6cm}|C{2cm}C{2cm}|C{2cm}C{2cm}| c}
    & \multicolumn{2}{c|}{Control sample} & \multicolumn{2}{c|}{Adapted sample}\\ 
    Repetition \# & Average execution time [ms] & Average CPU usage & Average execution time [ms] & Average CPU usage \\  \cline{1-5}
    1		  &3.59 & 44.30\% & 2.07 & 50.00\% \\ 
    2		  &3.70	& 45.00\% & 1.44 & 47.20\% \\
    3		  &3.24 & 48.20\% & 2.87 & 49.00\% \\
    4		  &3.14	& 44.60\% & 1.74 & 48.00\% \\
    5		  &3.41	& 44.10\% &	1.73 & 47.70\% \\
    6		  &3.06 & 43.40\% & 2.42 & 51.40\% \\
    7		  &3.40	& 62.70\% & 2.17 & 44.90\% \\
    8		  &2.98	& 44.20\% & 1.84 & 56.50\% \\
    9		  &3.13	& 48.20\% & 1.85 & 49.80\% \\
    10		  &3.05	& 47.90\% & 2.12 & 47.70\% \\ \cline{1-6}
        	  &3.27	& 47.26\% & 2.02 & 49.22\% & \multicolumn{1}{c}{Average} \\ 
\end{tabular}
\end{center}
\caption{System evaluation measurements - combined solution measurements}
\label{appendixmeasurementscombined}
\end{table}


\pagebreak

\subsection{System evaluation measurements - random traffic} \label{appendix:random}
{\renewcommand{\arraystretch}{1.4}
\begin{table}[ht]
\begin{center}
\begin{tabular}{c C{1.5cm} | C{1.4cm} C{1.4cm} C{1.4cm} C{1.5cm} C{1.5cm} C{1.4cm} | C{1.4cm} }
    & & \multicolumn{6}{c|}{Average Request Execution Time [ms]} & \multirow{2}{1.4cm}{Average CPU Usage} \\
                 \# & Sample & find & update & insert & wait & cpuTask & total &  \\ \hline 
 \multirow{2}{*}{1} & Control& 0.11 & 0.43   & 0.35   & 5.40 & 41.70   &     4.00          &     92\%   \\ 	  
                    & Adapted& 0.37 & 0.57   & 0.69   & 5.30 & 33.00   &     3.50          &     93\%   \\ \hline
 \multirow{2}{*}{2} & Control& 0.43 & 0.33   & 0.21   & 5.30 & 36.30   &     2.80          &     43\%   \\ 	  
                    & Adapted& 0.39 & 0.59   & 0.33   & 0.43 & 5.88    &     0.53          &     45\%   \\ \hline 
 \multirow{2}{*}{3} & Control& 0.36 & 0.27   & 0.59   & 5.35 & -       &     0.45          &     88\%   \\ 	  
                    & Adapted& 0.29 & 0.44   & 1.52   & 7.23 & -       &     0.53          &     69\%   \\ \hline 
 \multirow{2}{*}{4} & Control& 0.96 & 0.73   & -      & 5.80 & 38.10   &     16.00         &     96\%   \\ 	  
                    & Adapted& 0.14 & 0.69   & -      & 4.00 & 6.30    &     3.40          &     76\%   \\ \hline 
 \multirow{2}{*}{5} & Control& 0.81 & 1.60   & 0.94   & 5.50 & 60.10   &     3.40          &     67\%   \\ 	  
                    & Adapted& 0.63 & 0.40   & 0.49   & 3.10 & 32.80   &     1.90          &     68\%   \\ \hline 
 \multirow{2}{*}{6} & Control& -    & 0.27   & 1.10   & 5.20 & 18.90   &     6.60          &     50\%   \\ 	  
                    & Adapted& -    & 1.44   & 0.37   & 0.48 & 1.05    &     0.88          &     19\%   \\ \hline
 \multirow{2}{*}{7} & Control& 0.35 & 0.09   & 0.39   & 5.41 & -       &     2.20          &     86\%   \\ 	  
                    & Adapted& 0.41 & 0.05   & 1.10   & 0.59 & -       &     0.49          &     69\%   \\ \hline 
 \multirow{2}{*}{8} & Control& 0.29 & -      & -      & 5.54 & 26.15   &     3.22          &     75\%   \\ 	  
                    & Adapted& 0.32 & -      & -      & 0.38 & 0.05    &     0.35          &     54\%   \\ \hline 
 \multirow{2}{*}{9} & Control& 0.31 & 0.72   & 0.43   & 5.40 & 28.90   &     4.20          &     72\%   \\ 	  
                    & Adapted& 0.07 & 0.56   & 0.16   & 2.60 & 13.90   &     2.00          &     57\%   \\ \hline 
 \multirow{2}{*}{10}& Control& 0.91 & 0.88   & 1.20   & 6.20 & 69.19   &     2.30          &     96\%   \\ 	  
                    & Adapted& 0.35 & 0.46   & 0.63   & 4.80 & 11.30   &     1.40          &     95\%   \\         
\end{tabular}
\end{center}
\caption{System evaluation measurements - randomly generated traffic scenarios}
\label{appendixmeasurementscombined}
\end{table}
}


\end{document}